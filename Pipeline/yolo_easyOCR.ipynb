{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# YOLOv8 + EasyOCR",
   "id": "f950b929fddc6d1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Dependencies",
   "id": "97c74fac1ea0b4a0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-11T12:25:59.049497Z",
     "start_time": "2025-05-11T12:25:56.454166Z"
    }
   },
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "from models.EasyOCR.utils import AttrDict, AttnLabelConverter, CTCLabelConverter\n",
    "import yaml\n",
    "from torchvision import transforms\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ torchvision==0.20 is incompatible with torch==2.4.\n",
      "Run 'pip install torchvision==0.19' to fix torchvision or 'pip install -U torch torchvision' to update both.\n",
      "For a full compatibility table see https://github.com/pytorch/vision#installation\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "36c20a49557196a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:25:59.112790Z",
     "start_time": "2025-05-11T12:25:59.098155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Configuration for Detection ---\n",
    "TARGET_CLASS_NAME = \"license_plate\" # The string name of the class we want to detect with YOLO\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "CONFIDENCE_THRESHOLD = 0.25 # YOLO default is often 0.25, adjust as needed\n",
    "\n",
    "BOX_COLOR = (255, 0, 0) # Blue for YOLO car plates (to differentiate if needed)\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "TEXT_BG_COLOR = (0, 0, 0) # Black\n",
    "\n",
    "OCR_BOX_COLOR = (0, 255, 0) # Green for OCR text bounding box (if you draw it)\n",
    "OCR_TEXT_COLOR = (50, 200, 255)  # Light blue/yellow for OCR text\n",
    "OCR_TEXT_BG_COLOR = (0,0,0) # Black background for OCR text\n",
    "\n",
    "# TARGET_CLASS_ID will be determined after loading YOLO model and seeing its classes\n",
    "\n",
    "# --- Get Video Path  ---\n",
    "video_path = \"../data/video/53.mp4\"  # Or your desired video\n",
    "if video_path == '0':\n",
    "    video_source = 0 # Use webcam\n",
    "else:\n",
    "    video_source = video_path"
   ],
   "id": "79d2b207be76b8e7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load YOLO",
   "id": "de1e708af6796d3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:25:59.218587Z",
     "start_time": "2025-05-11T12:25:59.129053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "yolo_model_path = \"../models/best_yolov8.pt\"\n",
    "yolo_model = YOLO(yolo_model_path, task=\"detect\")"
   ],
   "id": "95efb180471aa275",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load EasyOCR",
   "id": "3ed93914392845dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:25:59.249335Z",
     "start_time": "2025-05-11T12:25:59.236056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_config(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    opt = AttrDict(opt)\n",
    "    if opt.lang_char == 'None':\n",
    "        characters = ''\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            csv_path = os.path.join(opt['train_data'], data, 'label.csv')\n",
    "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character= ''.join(characters)\n",
    "    else:\n",
    "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
    "    return opt"
   ],
   "id": "9d809b8cd23cfaf8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:25:59.280573Z",
     "start_time": "2025-05-11T12:25:59.265576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.EasyOCR.model import Model\n",
    "\n",
    "opt = get_config(\"../models/EasyOCR/opt.txt\")\n",
    "\n",
    "if not opt.data_filtering_off:\n",
    "    print('Filtering the images containing characters which are not in opt.character')\n",
    "    print('Filtering the images whose label is longer than opt.batch_max_length')\n",
    "\n",
    "opt.select_data = opt.select_data.split('-')\n",
    "opt.batch_ratio = opt.batch_ratio.split('-')\n",
    "\n",
    "if 'CTC' in opt.Prediction:\n",
    "    converter = CTCLabelConverter(opt.character)\n",
    "else:\n",
    "    converter = AttnLabelConverter(opt.character)\n",
    "opt.num_class = len(converter.character)\n",
    "\n",
    "if opt.rgb:\n",
    "    opt.input_channel = 3"
   ],
   "id": "88a26ca28daf78fe",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:25:59.419664Z",
     "start_time": "2025-05-11T12:25:59.296575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "easyocr = Model(opt)\n",
    "print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "      opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "      opt.SequenceModeling, opt.Prediction)\n",
    "easyocr = torch.nn.DataParallel(easyocr).to(device)\n",
    "easyocr.load_state_dict(torch.load('../models/best_easyocr_full.pth', map_location = 'cuda'), strict=False)\n",
    "easyocr.eval()"
   ],
   "id": "2adc52a5112dca56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Transformation module specified\n",
      "model input parameters 64 256 20 1 512 256 37 12 None VGG BiLSTM CTC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Model(\n",
       "    (FeatureExtraction): VGG_FeatureExtractor(\n",
       "      (ConvNet): Sequential(\n",
       "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "        (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (9): ReLU(inplace=True)\n",
       "        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "        (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (16): ReLU(inplace=True)\n",
       "        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "        (18): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (19): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
       "    (SequenceModeling): Sequential(\n",
       "      (0): BidirectionalLSTM(\n",
       "        (rnn): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
       "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): BidirectionalLSTM(\n",
       "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
       "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (Prediction): Linear(in_features=256, out_features=37, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Draw Predictions (Object Detection & OCR)",
   "id": "ef423991c95a0aa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:26:10.985669Z",
     "start_time": "2025-05-11T12:25:59.436308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell id: fc256f89ef200a19 (NEW CONTENT)\n",
    "\n",
    "# --- Video Capture ---\n",
    "cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "# To store YOLO class names and our target class ID for YOLO\n",
    "yolo_class_names_map = None\n",
    "yolo_target_cls_id_int = None\n",
    "\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video source '{video_source}'. Please check the path or camera.\")\n",
    "else:\n",
    "    print(f\"Processing video: {video_source} with YOLOv8 and Custom EasyOCR\")\n",
    "    window_title = f\"Car Plate YOLOv8 Detection & Custom EasyOCR\"\n",
    "    cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # Define the image transformation for the custom EasyOCR model\n",
    "    ocr_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((opt.imgH, opt.imgW)), # opt.imgH, opt.imgW from custom OCR config\n",
    "        transforms.Grayscale(num_output_channels=1), # Ensure grayscale, opt.input_channel is 1\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]) # For grayscale, normalizes [0,1] to [-1.0, 1.0]\n",
    "    ])\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame_bgr = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            if isinstance(video_source, str):\n",
    "                print(\"End of video file reached.\")\n",
    "            else:\n",
    "                print(\"Error reading frame from webcam.\")\n",
    "            break\n",
    "\n",
    "        frame_to_draw = frame_bgr.copy()\n",
    "        frame_count += 1\n",
    "\n",
    "        # --- YOLOv8 Inference ---\n",
    "        # source=frame_bgr (already BGR), stream=False for single image, verbose=False for less output\n",
    "        # device=device will use the torch device. YOLO handles moving data.\n",
    "        yolo_results = yolo_model(frame_bgr, device=device, verbose=False, conf=CONFIDENCE_THRESHOLD)\n",
    "\n",
    "        # Get results for the first (and only) image\n",
    "        result = yolo_results[0]\n",
    "\n",
    "        # --- Initialize YOLO class names and target ID on the first frame ---\n",
    "        if yolo_class_names_map is None:\n",
    "            yolo_class_names_map = result.names  # This is a dict like {0: 'person', 1: 'car', 2: 'carplate'}\n",
    "            print(f\"YOLO Model Classes: {yolo_class_names_map}\")\n",
    "            # Find the integer class ID for our TARGET_CLASS_NAME\n",
    "            for cls_id_int, name_str in yolo_class_names_map.items():\n",
    "                if name_str.lower() == TARGET_CLASS_NAME.lower():\n",
    "                    yolo_target_cls_id_int = cls_id_int\n",
    "                    break\n",
    "            if yolo_target_cls_id_int is None:\n",
    "                print(f\"Error: YOLO Target class '{TARGET_CLASS_NAME}' not found in YOLO model's classes: {yolo_class_names_map}\")\n",
    "                # Optionally, break or stop processing if target class isn't found\n",
    "            else:\n",
    "                print(f\"Targeting YOLO class '{TARGET_CLASS_NAME}' with integer ID: {yolo_target_cls_id_int}\")\n",
    "\n",
    "        # --- Process Detections and Perform OCR ---\n",
    "        num_detections_in_frame = 0\n",
    "        if yolo_target_cls_id_int is not None:\n",
    "            # Iterate through detected boxes\n",
    "            for box in result.boxes:\n",
    "                cls_id_tensor = box.cls # Tensor with the class ID\n",
    "                conf_tensor = box.conf # Tensor with the confidence score\n",
    "                xyxy_tensor = box.xyxy[0] # Tensor with [xmin, ymin, xmax, ymax]\n",
    "\n",
    "                cls_id_int = int(cls_id_tensor.item()) # Get Python int from tensor\n",
    "                confidence = conf_tensor.item() # Get Python float from tensor\n",
    "\n",
    "                if cls_id_int == yolo_target_cls_id_int: # No need for confidence check here, YOLO already filtered by `conf`\n",
    "                    num_detections_in_frame += 1\n",
    "                    xmin, ymin, xmax, ymax = map(int, xyxy_tensor.cpu().numpy()) # Get coords as integers\n",
    "\n",
    "                    # Clamp coordinates\n",
    "                    xmin = max(0, xmin); ymin = max(0, ymin)\n",
    "                    xmax = min(frame_bgr.shape[1], xmax); ymax = min(frame_bgr.shape[0], ymax)\n",
    "\n",
    "                    # Draw YOLO bounding box\n",
    "                    cv2.rectangle(frame_to_draw, (xmin, ymin), (xmax, ymax), BOX_COLOR, 2)\n",
    "                    yolo_label_text = f\"{yolo_class_names_map[cls_id_int]}: {confidence:.2f}\"\n",
    "\n",
    "                    (text_w_yolo, text_h_yolo), base_yolo = cv2.getTextSize(yolo_label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "                    cv2.rectangle(frame_to_draw, (xmin, ymin - text_h_yolo - base_yolo - 2), (xmin + text_w_yolo, ymin - base_yolo + 2), TEXT_BG_COLOR, -1)\n",
    "                    cv2.putText(frame_to_draw, yolo_label_text, (xmin, ymin - base_yolo -1),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, TEXT_COLOR, 1, cv2.LINE_AA)\n",
    "\n",
    "                    # --- Custom EasyOCR: Crop the plate and read text ---\n",
    "                    if xmax > xmin and ymax > ymin:\n",
    "                        plate_roi_bgr = frame_bgr[ymin:ymax, xmin:xmax]\n",
    "                        try:\n",
    "                            ocr_image_tensor = ocr_transform(plate_roi_bgr)\n",
    "                            ocr_image_tensor = ocr_image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "                            with torch.no_grad():\n",
    "                                # 'easyocr' is your custom OCR model instance from cell 2adc52a5112dca56\n",
    "                                ocr_preds_raw = easyocr(ocr_image_tensor, '')\n",
    "\n",
    "                            seq_len_T = ocr_preds_raw.size(1)\n",
    "                            if seq_len_T == 0:\n",
    "                                recognized_text = \"\"\n",
    "                            else:\n",
    "                                _, ocr_preds_idx_batched = ocr_preds_raw.max(2)\n",
    "                                ocr_preds_idx_flattened = ocr_preds_idx_batched.view(-1)\n",
    "                                batch_size_ocr = ocr_image_tensor.size(0)\n",
    "                                preds_size_ocr = torch.IntTensor([seq_len_T] * batch_size_ocr).to(device)\n",
    "\n",
    "                                # 'converter' is your custom OCR model's converter\n",
    "                                recognized_text_list = converter.decode_greedy(ocr_preds_idx_flattened.data, preds_size_ocr.data)\n",
    "                                recognized_text = recognized_text_list[0] if recognized_text_list else \"\"\n",
    "                                recognized_text = ''.join(filter(str.isalnum, recognized_text)).upper()\n",
    "\n",
    "                            if recognized_text:\n",
    "                                ocr_text_y_pos = ymax + 20\n",
    "                                if ocr_text_y_pos + 10 > frame_to_draw.shape[0]:\n",
    "                                    ocr_text_y_pos = ymin - 10\n",
    "\n",
    "                                (text_w_ocr, text_h_ocr), base_ocr = cv2.getTextSize(recognized_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "                                cv2.rectangle(frame_to_draw, (xmin, ocr_text_y_pos - text_h_ocr - base_ocr),\n",
    "                                              (xmin + text_w_ocr, ocr_text_y_pos + base_ocr),\n",
    "                                              OCR_TEXT_BG_COLOR, -1)\n",
    "                                cv2.putText(frame_to_draw, recognized_text, (xmin, ocr_text_y_pos),\n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, OCR_TEXT_COLOR, 2)\n",
    "                                if frame_count % 10 == 0:\n",
    "                                    print(f\"Frame {frame_count}: YOLO Plate (Conf: {confidence:.2f}), Custom OCR: '{recognized_text}'\")\n",
    "                        except Exception as e:\n",
    "                            if frame_count % 10 == 0 or \"Boolean value of Tensor\" in str(e):\n",
    "                                print(f\"Frame {frame_count}: Error during Custom OCR for a plate ROI: {e}\")\n",
    "\n",
    "        if frame_count % 30 == 0:\n",
    "            print(f\"Frame {frame_count}: Found {num_detections_in_frame} '{TARGET_CLASS_NAME}' instances by YOLOv8.\")\n",
    "\n",
    "        cv2.imshow(window_title, frame_to_draw)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "    if cap.isOpened():\n",
    "        cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Video processing finished and resources released.\")"
   ],
   "id": "fc256f89ef200a19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: data/video/53.mp4 with YOLOv8 and Custom EasyOCR\n",
      "YOLO Model Classes: {0: 'license_plate'}\n",
      "Targeting YOLO class 'license_plate' with integer ID: 0\n",
      "Frame 10: YOLO Plate (Conf: 0.76), Custom OCR: 'QCH2377'\n",
      "Frame 20: YOLO Plate (Conf: 0.62), Custom OCR: 'QCH2357'\n",
      "Frame 30: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 60: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 80: YOLO Plate (Conf: 0.44), Custom OCR: 'QAV951'\n",
      "Frame 90: YOLO Plate (Conf: 0.80), Custom OCR: 'QKV880'\n",
      "Frame 90: Found 1 'license_plate' instances by YOLOv8.\n",
      "Frame 100: YOLO Plate (Conf: 0.44), Custom OCR: 'QKV898'\n",
      "Frame 120: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 150: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 180: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 210: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 240: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 270: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 300: YOLO Plate (Conf: 0.84), Custom OCR: 'QAB7550'\n",
      "Frame 300: Found 1 'license_plate' instances by YOLOv8.\n",
      "Frame 330: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 360: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 380: YOLO Plate (Conf: 0.82), Custom OCR: 'SAR704'\n",
      "Frame 390: YOLO Plate (Conf: 0.76), Custom OCR: 'QQAM704'\n",
      "Frame 390: Found 1 'license_plate' instances by YOLOv8.\n",
      "Frame 400: YOLO Plate (Conf: 0.77), Custom OCR: 'QAM704'\n",
      "Frame 420: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 450: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 480: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 510: YOLO Plate (Conf: 0.78), Custom OCR: 'QPJ03'\n",
      "Frame 510: Found 1 'license_plate' instances by YOLOv8.\n",
      "Frame 520: YOLO Plate (Conf: 0.70), Custom OCR: 'P7823'\n",
      "Frame 530: YOLO Plate (Conf: 0.38), Custom OCR: 'QP71903'\n",
      "Frame 530: YOLO Plate (Conf: 0.37), Custom OCR: 'QAC1235'\n",
      "Frame 540: YOLO Plate (Conf: 0.77), Custom OCR: 'QACX2358'\n",
      "Frame 540: Found 1 'license_plate' instances by YOLOv8.\n",
      "Frame 550: YOLO Plate (Conf: 0.67), Custom OCR: 'CK2358'\n",
      "Frame 570: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 600: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 630: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 660: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 690: Found 0 'license_plate' instances by YOLOv8.\n",
      "Frame 720: Found 0 'license_plate' instances by YOLOv8.\n",
      "Exiting...\n",
      "Video processing finished and resources released.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
