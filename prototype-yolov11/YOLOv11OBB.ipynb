{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:23.633411Z",
     "start_time": "2025-03-19T09:57:23.629411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils import ops\n",
    "from ultralytics.utils.loss import v8OBBLoss\n",
    "from pathlib import Path"
   ],
   "id": "786cabf8b8baf10c",
   "outputs": [],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:23.649409Z",
     "start_time": "2025-03-19T09:57:23.637411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, data_yaml, transforms=None, mode='train'):\n",
    "        super().__init__()\n",
    "        self.data_yaml = data_yaml\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "\n",
    "        # Load YAML *and* check dataset\n",
    "        self.data_dict, self.img_files, self.label_files = self._load_and_prepare_data(data_yaml)\n",
    "        self.class_names = self.data_dict['names']\n",
    "        self.num_classes = len(self.class_names)\n",
    "\n",
    "    def _load_and_prepare_data(self, data_yaml_path):\n",
    "        \"\"\"Loads YAML, checks data, and constructs image and label file lists.\"\"\"\n",
    "        with open(data_yaml_path, 'r', errors='ignore') as f:\n",
    "            data_dict = yaml.safe_load(f)\n",
    "\n",
    "        # Ensure 'path' is an absolute path and a Path object\n",
    "        data_dict['path'] = Path(os.path.abspath(data_dict['path']))\n",
    "\n",
    "        # Check for required keys\n",
    "        for key in ('train', 'validation', 'names'):\n",
    "            if key not in data_dict:\n",
    "                raise ValueError(f\"'{key}' is missing from {data_yaml_path}\")\n",
    "\n",
    "        # --- Construct image and label file lists ---\n",
    "        if self.mode == 'train':\n",
    "            image_set = data_dict['train']\n",
    "        elif self.mode == 'val':\n",
    "            image_set = data_dict['validation']\n",
    "        elif self.mode == 'test':\n",
    "            image_set = data_dict['test']  # Handle test set\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'images', 'images', or 'images'\")\n",
    "\n",
    "        # Handle both directory-based and file-list-based datasets\n",
    "        img_files = []\n",
    "        label_files = []\n",
    "\n",
    "        if isinstance(image_set, str) and image_set.endswith('.txt'):  # File list\n",
    "            # Construct full paths for images\n",
    "            with open(data_dict['path'] / image_set, 'r') as f:\n",
    "                img_files = [str(data_dict['path'] / line.strip()) for line in f if line.strip()]\n",
    "            label_files = [img_file.replace('images', 'labels').replace(Path(img_file).suffix, '.txt')\n",
    "                           for img_file in img_files]\n",
    "\n",
    "        elif isinstance(image_set, str): # Directory\n",
    "            img_dir = data_dict['path'] / image_set\n",
    "            label_dir = data_dict['path'] / image_set.replace('images', 'labels')\n",
    "\n",
    "            if not os.path.exists(img_dir):\n",
    "                raise FileNotFoundError(f\"Image directory does not exist: {img_dir}\")\n",
    "            if not os.path.exists(label_dir):\n",
    "                raise FileNotFoundError(f\"Label directory does not exist: {label_dir}\")\n",
    "\n",
    "            img_files = sorted([str(f) for f in img_dir.glob('*') if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']])\n",
    "            label_files = [str(label_dir / Path(f).name.replace(Path(f).suffix, '.txt')) for f in img_files]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"'images', 'images' and 'images' must be a path to directory, or a path to .txt file\")\n",
    "\n",
    "\n",
    "        return data_dict, img_files, label_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_files[idx]\n",
    "        label_path = self.label_files[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img_width, img_height = image.size\n",
    "\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip().split()\n",
    "                    class_id = int(line[0])\n",
    "                    # Always treat as OBB\n",
    "                    coords = [float(c) for c in line[1:]]\n",
    "                    labels.append([class_id] + coords)\n",
    "\n",
    "\n",
    "        if len(labels) > 0:\n",
    "            labels = torch.tensor(labels)\n",
    "        else:\n",
    "            labels = torch.zeros((0, 9))  # Always 9 for OBB (x1, y1, ..., x4, y4)\n",
    "\n",
    "        if self.transforms:\n",
    "            # Apply transforms as a dictionary\n",
    "            sample = {'image': image, 'bboxes': labels[:, 1:], 'labels': labels[:, 0]}\n",
    "            sample = self.transforms(sample)\n",
    "            image = sample['image']\n",
    "            if len(sample['bboxes']) > 0:\n",
    "                labels = torch.cat((sample['labels'].unsqueeze(1), sample['bboxes']), dim=1)\n",
    "            else:\n",
    "                labels = torch.zeros((0, 9))  # Keep consistent OBB format\n",
    "        else:\n",
    "            image = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        return image, labels\n"
   ],
   "id": "8206dd6f30b18f0c",
   "outputs": [],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:23.857409Z",
     "start_time": "2025-03-19T09:57:23.842409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResizeAndPad:  # Custom transform\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size  # (width, height)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, bboxes, labels = sample['image'], sample['bboxes'], sample['labels']\n",
    "        # Resize and pad the image\n",
    "        w, h = image.size\n",
    "        scale = min(self.target_size[0] / w, self.target_size[1] / h)\n",
    "        new_w = int(w * scale)\n",
    "        new_h = int(h * scale)\n",
    "        image = F.resize(image, (new_h, new_w))\n",
    "\n",
    "        pad_w = self.target_size[0] - new_w\n",
    "        pad_h = self.target_size[1] - new_h\n",
    "        pad_left = pad_w // 2\n",
    "        pad_top = pad_h // 2\n",
    "\n",
    "        image = F.pad(image, (pad_left, pad_top, pad_w - pad_left, pad_h - pad_top))\n",
    "\n",
    "        # Adjust bounding box coordinates (always OBB)\n",
    "        if len(bboxes) > 0:\n",
    "            img_width, img_height = w, h\n",
    "            for i in range(0, bboxes.shape[1], 2):\n",
    "                bboxes[:, i] = (bboxes[:, i] * img_width * scale + pad_left) / self.target_size[0]\n",
    "                bboxes[:, i + 1] = (bboxes[:, i + 1] * img_height * scale + pad_top) / self.target_size[1]\n",
    "\n",
    "        return {'image': image, 'bboxes': bboxes, 'labels': labels}"
   ],
   "id": "9190b246b8aff7aa",
   "outputs": [],
   "execution_count": 220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:24.081411Z",
     "start_time": "2025-03-19T09:57:24.066411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ComposeTransforms:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for t in self.transforms:\n",
    "            if isinstance(t, (transforms.RandomHorizontalFlip, transforms.RandomVerticalFlip, transforms.ToTensor, transforms.Normalize)):\n",
    "                # Apply these transforms ONLY to the image part\n",
    "                sample['image'] = t(sample['image'])\n",
    "            else:\n",
    "                # Apply other transforms (like ResizeAndPad) to the whole sample\n",
    "                sample = t(sample)\n",
    "        return sample"
   ],
   "id": "e53faf9a61b14d0a",
   "outputs": [],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:24.304416Z",
     "start_time": "2025-03-19T09:57:24.292418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_yaml_path = './datasets/yolo_obb/data.yaml'  #  Path to your data.yaml file\n",
    "model_path = \"./yolo11n-obb.pt\"  #  Path to your YOLOv11 model"
   ],
   "id": "9511529d732ff90a",
   "outputs": [],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:24.523144Z",
     "start_time": "2025-03-19T09:57:24.508061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_transforms = ComposeTransforms([\n",
    "    ResizeAndPad((1920, 1088)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "val_transforms = ComposeTransforms([\n",
    "    ResizeAndPad((1920, 1088)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ],
   "id": "44e2200601a5835e",
   "outputs": [],
   "execution_count": 223
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:24.821651Z",
     "start_time": "2025-03-19T09:57:24.729657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = YOLODataset(data_yaml_path, transforms=train_transforms, mode='train')\n",
    "val_dataset = YOLODataset(data_yaml_path, transforms=val_transforms, mode='val')"
   ],
   "id": "83b0bbc31275a39d",
   "outputs": [],
   "execution_count": 224
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:25.029654Z",
     "start_time": "2025-03-19T09:57:25.014653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=lambda x: tuple(zip(*x)))"
   ],
   "id": "2327cb39c14ca343",
   "outputs": [],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:25.283444Z",
     "start_time": "2025-03-19T09:57:25.220444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "loaded_state = torch.load(model_path, map_location=device)\n",
    "model = loaded_state['model'].float()  # Load and convert to float\n",
    "model = model.to(device)"
   ],
   "id": "46069048a7d7f63d",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:25.503718Z",
     "start_time": "2025-03-19T09:57:25.489718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hyp = loaded_state.get('train_args', {}).copy()  # Use .get and .copy\n",
    "if hyp:\n",
    "    # Convert any 'True' or 'False' strings to boolean values\n",
    "    for k, v in hyp.items():\n",
    "        if v == 'True':\n",
    "            hyp[k] = True\n",
    "        elif v == 'False':\n",
    "            hyp[k] = False\n",
    "else:  # Handle situation that hyp is empty\n",
    "    hyp = {  # Set default values.  Adjust these as needed!\n",
    "        'lr0': 0.01,\n",
    "        'lrf': 0.01,\n",
    "        'momentum': 0.937,\n",
    "        'weight_decay': 0.0005,\n",
    "        'warmup_epochs': 3.0,\n",
    "        'warmup_momentum': 0.8,\n",
    "        'box': 7.5,\n",
    "        'cls': 0.5,\n",
    "        'dfl': 1.5,\n",
    "        'hsv_h': 0.015,\n",
    "        'hsv_s': 0.7,\n",
    "        'hsv_v': 0.4,\n",
    "        'degrees': 0.0,\n",
    "        'translate': 0.1,\n",
    "        'scale': 0.5,\n",
    "        'shear': 0.0,\n",
    "        'perspective': 0.0,\n",
    "        'flipud': 0.0,\n",
    "        'fliplr': 0.5,\n",
    "        'mosaic': 1.0,\n",
    "        'mixup': 0.0,\n",
    "        'copy_paste': 0.0,\n",
    "    }\n",
    "num_classes = model.model[-1].nc\n",
    "model.names = train_dataset.class_names  # Important: Set names\n",
    "model.nc = num_classes  # Important: Set number of classes"
   ],
   "id": "48159804e645195b",
   "outputs": [],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:25.721402Z",
     "start_time": "2025-03-19T09:57:25.708264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, optimizer, train_loader, device, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = torch.stack(list(image.to(device) for image in images))\n",
    "        targets = []\n",
    "        for i, label in enumerate(labels):\n",
    "            if len(label) == 0:\n",
    "                targets.append(torch.zeros((0, 10), device=device))\n",
    "                continue\n",
    "\n",
    "            # Add batch index to labels, and move to device\n",
    "            target = torch.cat((torch.ones(label.shape[0], 1, device=device) * i, label.to(device)), dim=1)\n",
    "            targets.append(target)\n",
    "        targets = torch.cat(targets, 0)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(images)  # Get predictions\n",
    "\n",
    "        # Prepare targets for Ultralytics' loss function\n",
    "        batch_size = images.shape[0]\n",
    "        targets_list = []\n",
    "        for i in range(batch_size):\n",
    "            indices = targets[:, 0] == i\n",
    "            targets_for_image = targets[indices, 1:]  # Remove batch index for loss calculation\n",
    "            targets_list.append(targets_for_image)\n",
    "\n",
    "        # Convert targets to the format expected by v8OBBLoss\n",
    "        targets_dict = {\n",
    "            'batch_idx': targets[:, 0],  # Batch indices\n",
    "            'cls': targets[:, 1],       # Class labels\n",
    "            'bboxes': targets[:, 2:],    # Bounding box coordinates (OBB format)\n",
    "            'imgsz': images.shape[2:]   # Pass image size here\n",
    "        }\n",
    "\n",
    "\n",
    "        # Calculate loss (using Ultralytics' DetectionLoss)\n",
    "        loss = criterion(preds, targets_dict)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n"
   ],
   "id": "f55381ad1a84b48b",
   "outputs": [],
   "execution_count": 228
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:25.942830Z",
     "start_time": "2025-03-19T09:57:25.928562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate(model, val_loader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = torch.stack(list(image.to(device) for image in images))\n",
    "            targets = []\n",
    "            for i, label in enumerate(labels):\n",
    "                if len(label) == 0:\n",
    "                    targets.append(torch.zeros((0, 10), device=device))  # Always 10 for OBB\n",
    "                    continue\n",
    "\n",
    "                # Always treat as OBB, move labels to device\n",
    "                target = torch.cat((torch.ones(label.shape[0], 1, device=device) * i, label.to(device)), dim=1)\n",
    "                targets.append(target)\n",
    "\n",
    "            targets = torch.cat(targets, 0)\n",
    "\n",
    "            # Forward Pass\n",
    "            preds = model(images)\n",
    "\n",
    "            # Prepare targets for Ultralytics' loss function (same as in training)\n",
    "            targets_dict = {\n",
    "                'batch_idx': targets[:, 0],\n",
    "                'cls': targets[:, 1],\n",
    "                'bboxes': targets[:, 2:],\n",
    "                'imgsz': images.shape[2:]\n",
    "            }\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(preds, targets_dict)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_loader)"
   ],
   "id": "a3d560912b010654",
   "outputs": [],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:26.164520Z",
     "start_time": "2025-03-19T09:57:26.150900Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = optim.Adam(model.parameters(), lr=float(hyp.get('lr0', 0.01))) # Use hyp",
   "id": "4c9c6a07aa0eef17",
   "outputs": [],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:26.386915Z",
     "start_time": "2025-03-19T09:57:26.371916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ultralytics.utils.loss import v8OBBLoss\n",
    "criterion = v8OBBLoss(model)"
   ],
   "id": "5a2874b7400b7204",
   "outputs": [],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:57:26.593852Z",
     "start_time": "2025-03-19T09:57:26.579603Z"
    }
   },
   "cell_type": "code",
   "source": "num_epochs = 1",
   "id": "5de08aba4b322c47",
   "outputs": [],
   "execution_count": 232
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device, criterion)\n",
    "    val_loss = validate(model, val_loader, device, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, images Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint (adapt as needed)\n",
    "    checkpoint_path = f'./checkpoints/yolov11_obb_epoch_{epoch+1}.pt'\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model': model,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, checkpoint_path)\n",
    "\n",
    "print(\"Training finished!\")\n"
   ],
   "id": "1c2af963d694e9d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
