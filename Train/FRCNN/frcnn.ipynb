{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:17.674767Z",
     "start_time": "2025-05-08T15:10:17.654278Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import pickle # For caching splits"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:20.963629Z",
     "start_time": "2025-05-08T15:10:17.865482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.ops import box_iou\n",
    "import torchvision.transforms.v2 as T # Use new v2 transforms API\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm.notebook import tqdm # Use notebook version for better display"
   ],
   "id": "d8f11eaed1df2baa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.024635Z",
     "start_time": "2025-05-08T15:10:21.009637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    from pycocotools.coco import COCO\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "    print(\"pycocotools found.\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: pycocotools not found. Evaluation will fail.\")\n",
    "    print(\"Install it: pip install pycocotools\")\n",
    "    COCO = None\n",
    "    COCOeval = None\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # Filter UserWarnings if they become noisy"
   ],
   "id": "4b4750271d43bd47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pycocotools found.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.055806Z",
     "start_time": "2025-05-08T15:10:21.041636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BASE_DATA_DIR = \"../../data\"\n",
    "FRAME_BASE_DIR = os.path.join(BASE_DATA_DIR, \"frame\")\n",
    "ANNOTATION_BASE_DIR = os.path.join(BASE_DATA_DIR, \"annotation\", \"coco\")\n",
    "MODEL_SAVE_DIR = \"../../models\"\n",
    "CACHE_DIR = \"../../cache\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ],
   "id": "858162262aa98498",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.086806Z",
     "start_time": "2025-05-08T15:10:21.071807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Splitting & Caching\n",
    "VIDEO_IDS = [f\"{i:02d}\" for i in range(1, 59)] # Assuming 01 to 58\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "# TEST_RATIO is implicitly 1.0 - TRAIN_RATIO - VAL_RATIO\n",
    "SPLIT_CACHE_FILE = os.path.join(CACHE_DIR, \"data_splits.pkl\")\n",
    "FORCE_REGENERATE_SPLITS = False # Set to True to recreate splits, otherwise load from cache if exists"
   ],
   "id": "9e64555912985596",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.149881Z",
     "start_time": "2025-05-08T15:10:21.103887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model & Training Hyperparameters\n",
    "NUM_CLASSES = 1 + 1 # Number of foreground classes + 1 background (UPDATE THIS based on your actual classes!)\n",
    "BATCH_SIZE = 16      # Adjust based on GPU memory\n",
    "NUM_EPOCHS = 25     # Number of training epochs\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0005\n",
    "LR_STEP_SIZE = 5    # Decrease LR every N epochs\n",
    "LR_GAMMA = 0.1      # Factor to decrease LR by\n",
    "NUM_WORKERS = 0     # DataLoader workers (adjust based on CPU cores)\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Number of object classes (excluding background): {NUM_CLASSES - 1}\")"
   ],
   "id": "8f2a5241b8006c39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of object classes (excluding background): 1\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.180164Z",
     "start_time": "2025-05-08T15:10:21.165778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if base directories exist\n",
    "if not os.path.isdir(FRAME_BASE_DIR) or not os.path.isdir(ANNOTATION_BASE_DIR):\n",
    "    raise FileNotFoundError(\"Frame or Annotation directory not found. Please check paths.\")"
   ],
   "id": "16ddc99911df7327",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.211600Z",
     "start_time": "2025-05-08T15:10:21.197164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_coco_metadata(annotation_dir, video_ids):\n",
    "    \"\"\"Loads image info and frame counts from multiple COCO JSON files.\"\"\"\n",
    "    all_image_info = [] # List of dicts: {'image_id': id, 'coco_file': path, 'img_info': {...}}\n",
    "    video_frame_counts = {}\n",
    "    categories = {} # {id: name}\n",
    "    cat_name_to_id = {} # {name: id} - Ensure consistency across files\n",
    "\n",
    "    print(f\"Loading metadata from {len(video_ids)} COCO files...\")\n",
    "    for video_id in tqdm(video_ids, desc=\"Loading JSONs\"):\n",
    "        coco_file = os.path.join(annotation_dir, f\"{video_id}.json\")\n",
    "        if not os.path.exists(coco_file):\n",
    "            print(f\"Warning: Annotation file not found: {coco_file}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            with open(coco_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            video_frame_counts[video_id] = len(data.get('images', []))\n",
    "\n",
    "            # Process categories - ensure consistency\n",
    "            if 'categories' in data:\n",
    "                for cat in data['categories']:\n",
    "                    if cat['id'] not in categories:\n",
    "                        categories[cat['id']] = cat['name']\n",
    "                        if cat['name'] not in cat_name_to_id:\n",
    "                             cat_name_to_id[cat['name']] = cat['id']\n",
    "                        elif cat_name_to_id[cat['name']] != cat['id']:\n",
    "                             print(f\"Warning: Category ID mismatch for '{cat['name']}' in {coco_file}. Using first encountered ID {cat_name_to_id[cat['name']]}.\")\n",
    "                    elif categories[cat['id']] != cat['name']:\n",
    "                         print(f\"Warning: Category Name mismatch for ID {cat['id']}' in {coco_file} ('{cat['name']}' vs '{categories[cat['id']]}'). Keeping first name.\")\n",
    "\n",
    "            # Store image references\n",
    "            for img in data.get('images', []):\n",
    "                # Ensure image id is unique globally if merging later, but here it's fine per file\n",
    "                # We store the original image ID from the JSON and the file it came from\n",
    "                all_image_info.append({\n",
    "                    'image_id_in_file': img['id'], # ID within its original JSON\n",
    "                    'coco_file': coco_file,\n",
    "                    'img_info': img # Store the whole image dict\n",
    "                })\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Error decoding JSON file: {coco_file}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing file {coco_file}: {e}. Skipping.\")\n",
    "\n",
    "    print(f\"Total images found across all files: {len(all_image_info)}\")\n",
    "    if not categories:\n",
    "         raise ValueError(\"No categories found in any annotation file. Cannot proceed.\")\n",
    "\n",
    "    # Create a sorted list of category names and a mapping to contiguous IDs (1-based)\n",
    "    sorted_cat_names = sorted(list(cat_name_to_id.keys()))\n",
    "    cat_name_to_contiguous_id = {name: i + 1 for i, name in enumerate(sorted_cat_names)} # 1-based index\n",
    "    print(\"\\nCategories found:\", cat_name_to_contiguous_id)\n",
    "\n",
    "    # Verify NUM_CLASSES matches detected categories\n",
    "    if NUM_CLASSES != (len(cat_name_to_contiguous_id) + 1):\n",
    "        print(f\"\\nWARNING: NUM_CLASSES configured ({NUM_CLASSES}) does not match detected categories ({len(cat_name_to_contiguous_id)} + 1 background).\")\n",
    "        print(\"Please update NUM_CLASSES in the configuration.\")\n",
    "        # Optionally raise error: raise ValueError(\"NUM_CLASSES mismatch\")\n",
    "\n",
    "    return all_image_info, video_frame_counts, cat_name_to_contiguous_id"
   ],
   "id": "63a911570ecf2221",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.758352Z",
     "start_time": "2025-05-08T15:10:21.228601Z"
    }
   },
   "cell_type": "code",
   "source": "all_image_references, video_frame_counts, category_map = load_coco_metadata(ANNOTATION_BASE_DIR, VIDEO_IDS)",
   "id": "d2ba79d71ef02122",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from 58 COCO files...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading JSONs:   0%|          | 0/58 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8de170b00af412c8a951105b44f196a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found across all files: 26002\n",
      "\n",
      "Categories found: {'Car Plate': 1}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.790351Z",
     "start_time": "2025-05-08T15:10:21.775351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_balanced_splits(video_ids, video_frame_counts, train_ratio, val_ratio):\n",
    "    \"\"\"Splits video IDs ensuring balanced frame counts.\"\"\"\n",
    "    num_videos = len(video_ids)\n",
    "    num_train = math.floor(train_ratio * num_videos)\n",
    "    num_val = math.floor(val_ratio * num_videos)\n",
    "    num_test = num_videos - num_train - num_val\n",
    "\n",
    "    print(f\"\\nTarget split sizes (by video count): Train={num_train}, Val={num_val}, Test={num_test}\")\n",
    "\n",
    "    # Shuffle videos for randomness\n",
    "    shuffled_video_ids = random.sample(video_ids, len(video_ids))\n",
    "\n",
    "    # Sort shuffled videos by frame count (desc) to help balance assignment\n",
    "    sorted_shuffled_videos = sorted(shuffled_video_ids, key=lambda vid: video_frame_counts.get(vid, 0), reverse=True)\n",
    "\n",
    "    train_vids, val_vids, test_vids = [], [], []\n",
    "    train_frames, val_frames, test_frames = 0, 0, 0\n",
    "\n",
    "    # Assign videos greedily to the split with the fewest frames currently\n",
    "    for video_id in sorted_shuffled_videos:\n",
    "        frames = video_frame_counts.get(video_id, 0)\n",
    "        assigned = False\n",
    "        # Prioritize filling smaller target sets first if counts are equal\n",
    "        split_counts = [\n",
    "            (train_frames, train_vids, num_train),\n",
    "            (val_frames, val_vids, num_val),\n",
    "            (test_frames, test_vids, num_test)\n",
    "        ]\n",
    "        # Sort splits by current frame count, then by remaining capacity (desc)\n",
    "        split_counts.sort(key=lambda x: (x[0], -(x[2] - len(x[1])) ))\n",
    "\n",
    "        for i in range(len(split_counts)):\n",
    "            current_frames, vid_list, target_count = split_counts[i]\n",
    "            if len(vid_list) < target_count:\n",
    "                 vid_list.append(video_id)\n",
    "                 # Update frame counts directly (list references mutable lists)\n",
    "                 if vid_list is train_vids: train_frames += frames\n",
    "                 elif vid_list is val_vids: val_frames += frames\n",
    "                 else: test_frames += frames\n",
    "                 assigned = True\n",
    "                 break\n",
    "\n",
    "        if not assigned: # Should not happen if ratios sum <= 1\n",
    "            print(f\"Warning: Could not assign video {video_id}. Adding to train set.\")\n",
    "            train_vids.append(video_id)\n",
    "            train_frames += frames\n",
    "\n",
    "\n",
    "    print(\"\\nActual split counts:\")\n",
    "    print(f\"  Train: {len(train_vids)} videos, {train_frames} frames\")\n",
    "    print(f\"  Val:   {len(val_vids)} videos, {val_frames} frames\")\n",
    "    print(f\"  Test:  {len(test_vids)} videos, {test_frames} frames\")\n",
    "\n",
    "    return set(train_vids), set(val_vids), set(test_vids)"
   ],
   "id": "922c7a65383f1a49",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.838351Z",
     "start_time": "2025-05-08T15:10:21.807352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not FORCE_REGENERATE_SPLITS and os.path.exists(SPLIT_CACHE_FILE):\n",
    "    print(f\"Loading cached splits from {SPLIT_CACHE_FILE}...\")\n",
    "    with open(SPLIT_CACHE_FILE, 'rb') as f:\n",
    "        split_data = pickle.load(f)\n",
    "    train_image_refs = split_data['train']\n",
    "    val_image_refs = split_data['val']\n",
    "    test_image_refs = split_data['test']\n",
    "    category_map = split_data['category_map'] # Load map from cache too\n",
    "    print(f\"Loaded splits: Train={len(train_image_refs)}, Val={len(val_image_refs)}, Test={len(test_image_refs)} images.\")\n",
    "    # Re-verify NUM_CLASSES from cached map\n",
    "    if NUM_CLASSES != (len(category_map) + 1):\n",
    "        print(f\"\\nWARNING: NUM_CLASSES configured ({NUM_CLASSES}) does not match cached category map ({len(category_map)} + 1 background). Using map from cache.\")\n",
    "        NUM_CLASSES = len(category_map) + 1\n",
    "\n",
    "else:\n",
    "    print(\"Generating new data splits...\")\n",
    "    train_vids, val_vids, test_vids = create_balanced_splits(\n",
    "        list(video_frame_counts.keys()), # Only use videos we found annotations for\n",
    "        video_frame_counts,\n",
    "        TRAIN_RATIO,\n",
    "        VAL_RATIO\n",
    "    )\n",
    "\n",
    "    # Filter all_image_references based on video ID splits\n",
    "    train_image_refs, val_image_refs, test_image_refs = [], [], []\n",
    "    for ref in all_image_references:\n",
    "        # Extract video ID from coco_file path\n",
    "        video_id = os.path.splitext(os.path.basename(ref['coco_file']))[0]\n",
    "        if video_id in train_vids:\n",
    "            train_image_refs.append(ref)\n",
    "        elif video_id in val_vids:\n",
    "            val_image_refs.append(ref)\n",
    "        elif video_id in test_vids:\n",
    "            test_image_refs.append(ref)\n",
    "\n",
    "    # Cache the splits\n",
    "    print(f\"\\nCaching splits to {SPLIT_CACHE_FILE}...\")\n",
    "    split_data_to_cache = {\n",
    "        'train': train_image_refs,\n",
    "        'val': val_image_refs,\n",
    "        'test': test_image_refs,\n",
    "        'category_map': category_map, # Save the category map used for this split\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    try:\n",
    "        with open(SPLIT_CACHE_FILE, 'wb') as f:\n",
    "            pickle.dump(split_data_to_cache, f)\n",
    "        print(\"Splits cached successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error caching splits: {e}\")"
   ],
   "id": "f1990de2c414542",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached splits from cache\\data_splits.pkl...\n",
      "Loaded splits: Train=12947, Val=6092, Test=6963 images.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.869282Z",
     "start_time": "2025-05-08T15:10:21.855284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sanity check\n",
    "print(f\"\\nFinal split sizes (images): Train={len(train_image_refs)}, Val={len(val_image_refs)}, Test={len(test_image_refs)}\")\n",
    "total_split = len(train_image_refs) + len(val_image_refs) + len(test_image_refs)\n",
    "print(f\"Total images in splits: {total_split} (Should match total found images if all videos were assigned)\")\n",
    "if total_split != len(all_image_references):\n",
    "    print(\"Warning: Total images in splits don't match initial count. Some video IDs might be missing from splits.\")"
   ],
   "id": "75210654bd9ebba9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final split sizes (images): Train=12947, Val=6092, Test=6963\n",
      "Total images in splits: 26002 (Should match total found images if all videos were assigned)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.900282Z",
     "start_time": "2025-05-08T15:10:21.885283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CocoMultiJsonDataset(Dataset):\n",
    "    def __init__(self, image_references, frame_base_dir, category_map, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_references (list): List of dicts {'image_id_in_file', 'coco_file', 'img_info'}.\n",
    "            frame_base_dir (str): Base directory where frame folders (01, 02...) are located.\n",
    "            category_map (dict): Mapping from category name to contiguous ID (1-based).\n",
    "            transforms (callable, optional): Transformations to apply.\n",
    "        \"\"\"\n",
    "        self.image_refs = image_references\n",
    "        self.frame_base_dir = frame_base_dir\n",
    "        self.transforms = transforms\n",
    "        self.category_map = category_map\n",
    "        self.cat_id_to_contiguous_id = {} # Map original category ID from JSON to contiguous ID\n",
    "\n",
    "        # Pre-load COCO objects or annotations per file for efficiency?\n",
    "        # For moderate number of files, loading on demand might be okay.\n",
    "        # If very slow, consider pre-loading into a dictionary:\n",
    "        self._coco_cache = {} # Cache loaded COCO objects or just annotations\n",
    "\n",
    "    def _get_coco_annotations(self, coco_file_path):\n",
    "        \"\"\"Loads annotations for a specific COCO file, caching results.\"\"\"\n",
    "        if coco_file_path not in self._coco_cache:\n",
    "            try:\n",
    "                with open(coco_file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                # Index annotations by image_id for faster lookup\n",
    "                ann_by_img_id = {}\n",
    "                for ann in data.get('annotations', []):\n",
    "                    img_id = ann['image_id']\n",
    "                    if img_id not in ann_by_img_id:\n",
    "                        ann_by_img_id[img_id] = []\n",
    "                    ann_by_img_id[img_id].append(ann)\n",
    "\n",
    "                 # Map original category IDs to contiguous IDs for this file\n",
    "                local_cat_id_map = {}\n",
    "                for cat in data.get('categories', []):\n",
    "                    cat_name = cat['name']\n",
    "                    if cat_name in self.category_map:\n",
    "                        local_cat_id_map[cat['id']] = self.category_map[cat_name]\n",
    "                    # else: # Should not happen if map is built correctly\n",
    "                    #     print(f\"Warning: Category '{cat_name}' from {coco_file_path} not in global map.\")\n",
    "\n",
    "                self._coco_cache[coco_file_path] = {\n",
    "                    'annotations': ann_by_img_id,\n",
    "                    'cat_id_map': local_cat_id_map\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading or processing COCO file {coco_file_path}: {e}\")\n",
    "                self._coco_cache[coco_file_path] = {'annotations': {}, 'cat_id_map': {}} # Mark as failed\n",
    "\n",
    "        return self._coco_cache[coco_file_path]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_refs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_ref = self.image_refs[idx]\n",
    "        img_metadata = img_ref['img_info']\n",
    "        coco_file = img_ref['coco_file']\n",
    "        img_id_in_file = img_ref['image_id_in_file'] # Use the ID specific to that JSON\n",
    "\n",
    "        # Construct image path (handle potential path separators)\n",
    "        # img_metadata['file_name'] might be like \"03\\\\frame_000000.jpg\" or \"03/frame_000000.jpg\"\n",
    "        relative_img_path = img_metadata['file_name'].replace('\\\\', os.sep).replace('/', os.sep)\n",
    "        img_path = os.path.join(self.frame_base_dir, relative_img_path)\n",
    "\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Error: Image file not found at {img_path}\")\n",
    "             # Return dummy data or raise error, depending on desired behavior\n",
    "             # Returning dummy data to avoid crashing DataLoader completely\n",
    "             w, h = img_metadata.get('width', 64), img_metadata.get('height', 64) # Use metadata size if possible\n",
    "             return T.ToTensor()(Image.new('RGB', (w, h))), {'boxes': torch.empty((0, 4)), 'labels': torch.empty(0, dtype=torch.int64)}\n",
    "\n",
    "\n",
    "        # Load annotations for this image from its corresponding COCO file\n",
    "        coco_data = self._get_coco_annotations(coco_file)\n",
    "        annotations = coco_data['annotations'].get(img_id_in_file, [])\n",
    "        local_cat_id_map = coco_data['cat_id_map']\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for ann in annotations:\n",
    "            # Convert COCO bbox [xmin, ymin, width, height] to [xmin, ymin, xmax, ymax]\n",
    "            xmin = ann['bbox'][0]\n",
    "            ymin = ann['bbox'][1]\n",
    "            xmax = xmin + ann['bbox'][2]\n",
    "            ymax = ymin + ann['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            # Map original category ID to contiguous ID\n",
    "            original_cat_id = ann['category_id']\n",
    "            if original_cat_id in local_cat_id_map:\n",
    "                 labels.append(local_cat_id_map[original_cat_id])\n",
    "            else:\n",
    "                 # Should we skip this annotation or assign a default? Skipping for now.\n",
    "                 # print(f\"Warning: Annotation with unknown category ID {original_cat_id} in image {img_id_in_file} from {coco_file}\")\n",
    "                 boxes.pop() # Remove the corresponding box\n",
    "                 continue # Skip this annotation\n",
    "\n",
    "            areas.append(ann.get('area', ann['bbox'][2] * ann['bbox'][3]))\n",
    "            iscrowd.append(ann.get('iscrowd', 0))\n",
    "\n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.uint8)\n",
    "        image_id = torch.tensor([idx]) # Use the index in the *current dataset split* as the image_id for evaluation\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id # Important for COCO eval\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms is not None:\n",
    "            # The new transforms handle image and target dicts together\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        # Ensure boxes are valid after transforms (e.g., width/height > 0)\n",
    "        if \"boxes\" in target:\n",
    "             boxes = target['boxes']\n",
    "             if boxes.shape[0] > 0:\n",
    "                 # Calculate width and height\n",
    "                 widths = boxes[:, 2] - boxes[:, 0]\n",
    "                 heights = boxes[:, 3] - boxes[:, 1]\n",
    "                 # Keep only boxes with positive width and height\n",
    "                 keep = (widths > 0) & (heights > 0)\n",
    "                 if not torch.all(keep):\n",
    "                     # print(f\"Warning: Found invalid boxes after transform for image index {idx}. Filtering.\")\n",
    "                     target['boxes'] = target['boxes'][keep]\n",
    "                     target['labels'] = target['labels'][keep]\n",
    "                     if 'area' in target: target['area'] = target['area'][keep]\n",
    "                     if 'iscrowd' in target: target['iscrowd'] = target['iscrowd'][keep]\n",
    "\n",
    "\n",
    "        return image, target"
   ],
   "id": "f6a72feb37047dbe",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.932285Z",
     "start_time": "2025-05-08T15:10:21.917284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "\n",
    "    # --- Add this line ---\n",
    "    # Explicitly convert PIL Image to PyTorch Tensor (dtype=uint8, range [0, 255])\n",
    "    transforms.append(T.PILToTensor())\n",
    "    # --- End of Addition ---\n",
    "\n",
    "    if train:\n",
    "        # Standard augmentation for detection\n",
    "        transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
    "        # Add other augmentations if needed (e.g., color jitter, geometric)\n",
    "        # transforms.append(T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1))\n",
    "\n",
    "    # Convert tensor dtype to float and scale to [0, 1]\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True)) # scale=True divides by 255\n",
    "\n",
    "    # Ensure output is pure tensor (might be slightly redundant after ToDtype, but good practice)\n",
    "    transforms.append(T.ToPureTensor())\n",
    "\n",
    "    return T.Compose(transforms)"
   ],
   "id": "14e04bbea12fee58",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.964283Z",
     "start_time": "2025-05-08T15:10:21.949284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_train = CocoMultiJsonDataset(train_image_refs, FRAME_BASE_DIR, category_map, get_transform(train=True))\n",
    "dataset_val = CocoMultiJsonDataset(val_image_refs, FRAME_BASE_DIR, category_map, get_transform(train=False))\n",
    "dataset_test = CocoMultiJsonDataset(test_image_refs, FRAME_BASE_DIR, category_map, get_transform(train=False))\n",
    "\n",
    "print(f\"\\nDataset sizes: Train={len(dataset_train)}, Val={len(dataset_val)}, Test={len(dataset_test)}\")"
   ],
   "id": "673e615e2f9a14d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset sizes: Train=12947, Val=6092, Test=6963\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:21.996282Z",
     "start_time": "2025-05-08T15:10:21.981284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    # Standard collate function for detection: returns tuple(list of images, list of targets)\n",
    "    return tuple(zip(*batch))"
   ],
   "id": "c6010c981cb4c632",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:22.026283Z",
     "start_time": "2025-05-08T15:10:22.012284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_loader_train = DataLoader(\n",
    "    dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn, pin_memory=torch.cuda.is_available() # Use pin_memory with GPU\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn, pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "data_loader_test = DataLoader(\n",
    "    dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn, pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created with Batch Size={BATCH_SIZE}, Num Workers={NUM_WORKERS}\")"
   ],
   "id": "48c488267624b527",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataLoaders created with Batch Size=16, Num Workers=0\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:22.072283Z",
     "start_time": "2025-05-08T15:10:22.058284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_faster_rcnn_model(num_classes):\n",
    "    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model, weights"
   ],
   "id": "864b5f14dc185bc0",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:22.514814Z",
     "start_time": "2025-05-08T15:10:22.089285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model, model_weights = get_faster_rcnn_model(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(model)"
   ],
   "id": "7ca26c822067b24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0-3): 4 x Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:22.546009Z",
     "start_time": "2025-05-08T15:10:22.531223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "def configure_optimizer_with_freezing(\n",
    "        model: torchvision.models.detection.FasterRCNN,\n",
    "        strategy: str,\n",
    "        lr: float,\n",
    "        weight_decay: float\n",
    ") -> Tuple[torch.optim.Optimizer, List[torch.nn.Parameter]]:\n",
    "    \"\"\"\n",
    "    Configures the optimizer for the model based on a freezing strategy.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model (Faster R-CNN).\n",
    "        strategy: A string defining the freezing strategy.\n",
    "                  Options: \"freeze_backbone\", \"predictor_only\", \"train_all\".\n",
    "        lr: Learning rate for the optimizer.\n",
    "        weight_decay: Weight decay for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the configured optimizer and the list of parameters\n",
    "        that will be optimized.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Applying freezing strategy: {strategy} ---\")\n",
    "\n",
    "    # Default: all parameters are trainable (important if model is reused)\n",
    "    # Or, ensure all are trainable before selectively freezing.\n",
    "    # For a freshly loaded model with a new head, the new head's params are True,\n",
    "    # and pretrained are True.\n",
    "    # To be safe and explicit:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True # Start with all trainable\n",
    "\n",
    "    if strategy == \"freeze_backbone\":\n",
    "        print(\"Freezing backbone. RPN and RoI heads (including predictor) will be trained.\")\n",
    "        if hasattr(model, 'backbone'):\n",
    "            for param in model.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            print(\"Warning: Model does not have a 'backbone' attribute to freeze.\")\n",
    "\n",
    "    elif strategy == \"predictor_only\":\n",
    "        print(\"Freezing all layers except the RoI heads box_predictor.\")\n",
    "        # First, freeze all parameters\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Then, unfreeze only the box_predictor parameters\n",
    "        if hasattr(model, 'roi_heads') and hasattr(model.roi_heads, 'box_predictor'):\n",
    "            for param in model.roi_heads.box_predictor.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            print(\"Warning: Model does not have 'roi_heads.box_predictor' to unfreeze.\")\n",
    "\n",
    "    elif strategy == \"train_all\":\n",
    "        print(\"All model parameters will be trained (no layers frozen).\")\n",
    "        # All parameters already set to requires_grad = True at the start of the function\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown freezing strategy: {strategy}. \"\n",
    "                         \"Choose from 'freeze_backbone', 'predictor_only', 'train_all'.\")\n",
    "\n",
    "    # Collect parameters that require gradients\n",
    "    params_to_optimize = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    # Print statistics\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in params_to_optimize)\n",
    "    print(f\"Total parameters in the model: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "    if strategy == \"predictor_only\" and hasattr(model, 'roi_heads') and hasattr(model.roi_heads, 'box_predictor'):\n",
    "        predictor_params = sum(p.numel() for p in model.roi_heads.box_predictor.parameters())\n",
    "        print(f\"Parameters in box_predictor: {predictor_params}\")\n",
    "        if trainable_params != predictor_params:\n",
    "            print(f\"Warning: Trainable params ({trainable_params}) do not match predictor params ({predictor_params}) for 'predictor_only' strategy.\")\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.AdamW(params_to_optimize, lr=lr, weight_decay=weight_decay)\n",
    "    print(\"Optimizer configured.\")\n",
    "\n",
    "    # Optional: print status of a few parameters\n",
    "    print(\"\\nSample parameter requires_grad status:\")\n",
    "    count = 0\n",
    "    shown_predictor = False\n",
    "    shown_backbone_frozen = False\n",
    "    shown_backbone_trainable = False\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        is_predictor = \"roi_heads.box_predictor\" in name\n",
    "        is_backbone = \"backbone\" in name\n",
    "\n",
    "        if count < 2: # Show first few global params\n",
    "            print(f\"  {name}: requires_grad={param.requires_grad}\")\n",
    "            count += 1\n",
    "        elif is_predictor and not shown_predictor:\n",
    "            print(f\"  {name} (predictor): requires_grad={param.requires_grad}\")\n",
    "            shown_predictor = True\n",
    "        elif is_backbone and not param.requires_grad and not shown_backbone_frozen:\n",
    "            print(f\"  {name} (backbone frozen): requires_grad={param.requires_grad}\")\n",
    "            shown_backbone_frozen = True\n",
    "        elif is_backbone and param.requires_grad and not shown_backbone_trainable and strategy != \"freeze_backbone\":\n",
    "            print(f\"  {name} (backbone trainable): requires_grad={param.requires_grad}\")\n",
    "            shown_backbone_trainable = True\n",
    "\n",
    "        if shown_predictor and (shown_backbone_frozen or strategy != \"freeze_backbone\") and (shown_backbone_trainable or strategy == \"freeze_backbone\") and count >=2:\n",
    "            if strategy == \"freeze_backbone\" and not shown_backbone_frozen: continue # Need to see a frozen backbone\n",
    "            if strategy != \"freeze_backbone\" and not shown_backbone_trainable: continue # Need to see a trainable backbone if not freezing it\n",
    "            break # Stop after showing a representative set\n",
    "\n",
    "    return optimizer, params_to_optimize"
   ],
   "id": "934e40a8a5703c06",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Freezing",
   "id": "417c8fa876fc224e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:22.576963Z",
     "start_time": "2025-05-08T15:10:22.561962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# strategy = \"freeze_backbone\"\n",
    "strategy = \"predictor_only\"\n",
    "# strategy = \"train_all\""
   ],
   "id": "381e9cf54bb4445e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:22.608179Z",
     "start_time": "2025-05-08T15:10:22.593178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer, params = configure_optimizer_with_freezing(\n",
    "    model,\n",
    "    strategy=strategy,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")"
   ],
   "id": "6e0332ac264761d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying freezing strategy: predictor_only ---\n",
      "Freezing all layers except the RoI heads box_predictor.\n",
      "Total parameters in the model: 41299161\n",
      "Trainable parameters: 10250\n",
      "Parameters in box_predictor: 10250\n",
      "Optimizer configured.\n",
      "\n",
      "Sample parameter requires_grad status:\n",
      "  backbone.body.conv1.weight: requires_grad=False\n",
      "  backbone.body.layer1.0.conv1.weight: requires_grad=False\n",
      "  backbone.body.layer1.0.conv2.weight (backbone frozen): requires_grad=False\n",
      "  roi_heads.box_predictor.cls_score.weight (predictor): requires_grad=True\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:22.732414Z",
     "start_time": "2025-05-08T15:10:22.717411Z"
    }
   },
   "cell_type": "code",
   "source": "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEP_SIZE, gamma=LR_GAMMA)",
   "id": "5faf3d76e99789f",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "c247c8c91be10c8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:22.762883Z",
     "start_time": "2025-05-08T15:10:22.748885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train() # Set model to training mode\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    # header = f'Epoch: [{epoch}]' # No longer strictly needed for console output with tqdm\n",
    "\n",
    "    # Setup tqdm progress bar\n",
    "    # `leave=False` is important if this function is called within another tqdm loop (e.g., epoch loop)\n",
    "    # so that this batch progress bar gets removed after completion.\n",
    "    progress_bar = tqdm(\n",
    "        data_loader,\n",
    "        desc=f\"Epoch {epoch+1} Training\",\n",
    "        total=len(data_loader),\n",
    "        leave=False, # Remove bar once done\n",
    "        dynamic_ncols=True # Adjust to terminal width\n",
    "    )\n",
    "\n",
    "    for i, (images, targets) in enumerate(progress_bar):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        try:\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # In a DDP scenario, you would reduce losses here.\n",
    "            # For single GPU, loss_dict_reduced is loss_dict, losses_reduced is losses.\n",
    "            loss_dict_reduced = {k: v.item() for k, v in loss_dict.items()} # Store scalar values for logger\n",
    "            losses_reduced = losses.item() # Store scalar value for logger\n",
    "\n",
    "            if not math.isfinite(losses_reduced):\n",
    "                tqdm.write(f\"Loss is {losses_reduced}, stopping training (Epoch {epoch+1}, Batch {i}).\") # Use tqdm.write\n",
    "                tqdm.write(str({k: v for k,v in loss_dict.items()})) # Print original tensor losses\n",
    "                return None # Signal failure\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update MetricLogger\n",
    "            metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "            metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "            # Update tqdm postfix with current averaged metrics from MetricLogger\n",
    "            # Displaying global_avg for more stability in the progress bar\n",
    "            postfix_data = {\n",
    "                'Loss': f\"{metric_logger.loss.global_avg:.4f}\",\n",
    "                'LR': f\"{metric_logger.lr.value:.6f}\" # Current LR\n",
    "            }\n",
    "            # Optionally add other losses if they are in loss_dict_reduced and you want to see them\n",
    "            # e.g., if 'loss_classifier' is a key:\n",
    "            if 'loss_classifier' in metric_logger.meters:\n",
    "                postfix_data['Cls Loss'] = f\"{metric_logger.loss_classifier.global_avg:.4f}\"\n",
    "            if 'loss_box_reg' in metric_logger.meters:\n",
    "                postfix_data['Box Loss'] = f\"{metric_logger.loss_box_reg.global_avg:.4f}\"\n",
    "\n",
    "\n",
    "            progress_bar.set_postfix(postfix_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Error during training (Epoch {epoch+1}, Batch {i}): {e}\") # Use tqdm.write\n",
    "            tqdm.write(\"Skipping this batch.\")\n",
    "            # It's important to zero_grad here if an error occurred after .backward() but before .step()\n",
    "            # or if the error was in .step() itself to prevent issues with the next batch.\n",
    "            # However, if the error is elsewhere, this might not be necessary.\n",
    "            # For safety, zeroing grad is often a good idea if continuing.\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "            except: # Handle cases where optimizer might not be fully initialized or in a weird state\n",
    "                pass\n",
    "            continue # Continue to the next batch\n",
    "\n",
    "    return metric_logger"
   ],
   "id": "edbca86105594ef6",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:22.809597Z",
     "start_time": "2025-05-08T15:10:22.794598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.inference_mode() # More efficient than torch.no_grad() for inference\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = 'Validate:'\n",
    "    coco_gt = None # Ground truth COCO object (built from validation set)\n",
    "    coco_predictions = [] # List to store predictions in COCO format\n",
    "\n",
    "    # Check if pycocotools is available\n",
    "    use_coco_eval = COCO is not None and COCOeval is not None\n",
    "\n",
    "    if use_coco_eval:\n",
    "        # Need to build a temporary COCO ground truth object from our validation dataset\n",
    "        # This is a bit inefficient but necessary for pycocotools\n",
    "        print(\"Building temporary COCO ground truth for validation evaluation...\")\n",
    "        val_gt_data = {'images': [], 'annotations': [], 'categories': []}\n",
    "        # Use the actual category map\n",
    "        for name, id_val in category_map.items():\n",
    "             val_gt_data['categories'].append({'id': id_val, 'name': name, 'supercategory': 'object'})\n",
    "\n",
    "        ann_id_counter = 1\n",
    "        img_id_map = {} # Map dataset index to a unique image id for COCO eval\n",
    "        current_img_id = 0\n",
    "\n",
    "        for i, (_, targets_batch) in enumerate(tqdm(data_loader, desc=\"Building Val GT\")):\n",
    "             for targets in targets_batch: # Process each target dict in the batch\n",
    "                 dataset_img_idx = targets['image_id'].item() # Get original dataset index\n",
    "                 if dataset_img_idx not in img_id_map:\n",
    "                     img_id_map[dataset_img_idx] = current_img_id\n",
    "                     # Find original image info (less efficient this way, could store in dataset item)\n",
    "                     original_ref = data_loader.dataset.image_refs[dataset_img_idx]\n",
    "                     img_info = original_ref['img_info']\n",
    "                     val_gt_data['images'].append({\n",
    "                         'id': current_img_id,\n",
    "                         'width': img_info.get('width', 0), # Get dimensions if available\n",
    "                         'height': img_info.get('height', 0),\n",
    "                         'file_name': img_info.get('file_name', f'img_{current_img_id}')\n",
    "                     })\n",
    "                     current_img_id += 1\n",
    "\n",
    "                 coco_img_id = img_id_map[dataset_img_idx]\n",
    "\n",
    "                 boxes = targets['boxes'].cpu().numpy()\n",
    "                 labels = targets['labels'].cpu().numpy()\n",
    "                 areas = targets.get('area', torch.zeros(len(boxes))).cpu().numpy() # Handle missing area\n",
    "\n",
    "                 for j in range(boxes.shape[0]):\n",
    "                     bbox = boxes[j]\n",
    "                     # Convert [xmin, ymin, xmax, ymax] back to [xmin, ymin, width, height]\n",
    "                     coco_bbox = [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]]\n",
    "                     val_gt_data['annotations'].append({\n",
    "                         'id': ann_id_counter,\n",
    "                         'image_id': coco_img_id,\n",
    "                         'category_id': labels[j],\n",
    "                         'bbox': coco_bbox,\n",
    "                         'area': areas[j],\n",
    "                         'iscrowd': targets['iscrowd'][j].cpu().item(),\n",
    "                     })\n",
    "                     ann_id_counter += 1\n",
    "\n",
    "        # Create COCO object from the built ground truth\n",
    "        if val_gt_data['annotations']:\n",
    "            coco_gt = COCO()\n",
    "            coco_gt.dataset = val_gt_data\n",
    "            coco_gt.createIndex()\n",
    "            print(\"Validation ground truth COCO object created.\")\n",
    "        else:\n",
    "            print(\"Warning: No ground truth annotations found for validation set. COCO evaluation will be skipped.\")\n",
    "            use_coco_eval = False\n",
    "\n",
    "\n",
    "    # --- Actual Evaluation Loop ---\n",
    "    val_loss_total = 0.0\n",
    "    val_batches = 0\n",
    "    print(\"\\nRunning validation inference...\")\n",
    "    for images, targets in metric_logger.log_every(data_loader, 50, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "        model.train() # Temp switch to get losses\n",
    "        with torch.inference_mode(): # Still disable gradients\n",
    "            loss_dict = model(images, targets)\n",
    "        model.eval() # Switch back to eval for predictions\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        val_loss_total += losses.item()\n",
    "        val_batches += 1\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        outputs = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in outputs]\n",
    "\n",
    "        # Format predictions for COCOeval\n",
    "        if use_coco_eval and coco_gt:\n",
    "            # Map dataset index back to the temporary COCO image ID\n",
    "            res = []\n",
    "            for i, output in enumerate(outputs):\n",
    "                original_dataset_idx = targets[i]['image_id'].item()\n",
    "                if original_dataset_idx in img_id_map:\n",
    "                     coco_image_id = img_id_map[original_dataset_idx]\n",
    "                     for box, label, score in zip(output['boxes'], output['labels'], output['scores']):\n",
    "                         if score > 0.6:\n",
    "                             bbox = box.tolist()\n",
    "                             coco_bbox = [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]]\n",
    "                             res.append({\n",
    "                                 \"image_id\": coco_image_id,\n",
    "                                 \"category_id\": label.item(),\n",
    "                                 \"bbox\": coco_bbox,\n",
    "                                 \"score\": score.item(),\n",
    "                             })\n",
    "            coco_predictions.extend(res)\n",
    "\n",
    "        # Log validation loss if calculated\n",
    "        metric_logger.update(val_loss=losses)\n",
    "\n",
    "    # --- COCO Evaluation ---\n",
    "    eval_summary = None\n",
    "    if use_coco_eval and coco_gt and coco_predictions:\n",
    "        print(\"\\nRunning COCO evaluation on validation predictions...\")\n",
    "        try:\n",
    "            coco_dt = coco_gt.loadRes(coco_predictions)\n",
    "            coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "            coco_eval.evaluate()\n",
    "            coco_eval.accumulate()\n",
    "            coco_eval.summarize()\n",
    "            eval_summary = coco_eval.stats # Store summary stats (mAP etc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during COCO evaluation: {e}\")\n",
    "            eval_summary = None\n",
    "    elif use_coco_eval:\n",
    "         print(\"Skipping COCO evaluation: Ground truth or predictions missing.\")\n",
    "\n",
    "\n",
    "    # Gather all stats from metric_logger\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print('Validation Result: {}'.format(metric_logger))\n",
    "\n",
    "    avg_val_loss = val_loss_total / val_batches if val_batches > 0 else float('inf')\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return avg_val_loss, eval_summary # Return loss and coco stats"
   ],
   "id": "71e6019cafffc4fe",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T15:10:22.840933Z",
     "start_time": "2025-05-08T15:10:22.826452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import collections\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = collections.deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        # Placeholder for distributed training, does nothing here\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count if self.count > 0 else float('nan')\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.count == 0: return \"N/A\" # Handle empty case\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = collections.defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            if not isinstance(v, (float, int)):\n",
    "                raise TypeError(\n",
    "                    \"This logger accepts only int or float values but received {} of type {}\".format(v, type(v))\n",
    "                )\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\"{}: {}\".format(name, str(meter)))\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n"
   ],
   "id": "7381b099cf6b270",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-08T15:10:22.872797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Starting Training ---\")\n",
    "start_training_time = time.time()\n",
    "\n",
    "# Determine base filename prefix based on strategy\n",
    "if strategy == \"freeze_backbone\":\n",
    "    base_filename_prefix = 'frcnn_freeze_backbone'\n",
    "elif strategy == \"predictor_only\":\n",
    "    base_filename_prefix = 'frcnn_predictor_only'\n",
    "else: # \"train_all\" or any other default\n",
    "    base_filename_prefix = 'frcnn_train_all'\n",
    "\n",
    "best_model_path = os.path.join(MODEL_SAVE_DIR, f\"best_{base_filename_prefix}.pth\")\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "best_val_metric_for_loss = float('inf') # Lower loss is better\n",
    "best_val_map = 0.0\n",
    "use_map_for_best = COCO is not None # Use mAP if pycocotools is available\n",
    "\n",
    "# Store history\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_map': []}\n",
    "\n",
    "# tqdm for the outer epoch loop\n",
    "epoch_pbar = tqdm(range(NUM_EPOCHS), desc=\"Overall Training Progress\")\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    epoch_pbar.set_description(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "    # Train for one epoch\n",
    "    # If train_one_epoch itself uses tqdm, it should ideally set leave=False\n",
    "    # for its progress bar or print to tqdm.write\n",
    "    train_logger = train_one_epoch(model, optimizer, data_loader_train, DEVICE, epoch)\n",
    "\n",
    "    if train_logger is None: # Check if training failed (e.g., due to high loss as per original code)\n",
    "        tqdm.write(\"Stopping training due to an issue in train_one_epoch (e.g., high loss).\")\n",
    "        break\n",
    "\n",
    "    # Update the learning rate\n",
    "    if lr_scheduler is not None: # Ensure lr_scheduler exists\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    # If evaluate itself uses tqdm, it should also set leave=False\n",
    "    avg_val_loss, coco_stats = evaluate(model, data_loader_val, DEVICE)\n",
    "\n",
    "    # Store history (use global averages from loggers)\n",
    "    current_train_loss = train_logger.meters['loss'].global_avg\n",
    "    history['train_loss'].append(current_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "    if coco_stats is not None and len(coco_stats) > 0:\n",
    "        val_map_50_95 = coco_stats[0]  # mAP @ IoU=0.50:0.95\n",
    "    else:\n",
    "        print(\"Warning: COCO evaluation failed. Using best validation loss instead.\")\n",
    "        val_map_50_95 = 0.0\n",
    "\n",
    "    history['val_map'].append(val_map_50_95)\n",
    "\n",
    "    # Update tqdm postfix with current epoch's metrics\n",
    "    postfix_metrics = {\n",
    "        'Train Loss': f\"{current_train_loss:.4f}\",\n",
    "        'Val Loss': f\"{avg_val_loss:.4f}\"\n",
    "    }\n",
    "    if use_map_for_best:\n",
    "        postfix_metrics['Val mAP'] = f\"{val_map_50_95:.4f}\"\n",
    "        postfix_metrics['Best mAP'] = f\"{best_val_map:.4f}\"\n",
    "    else:\n",
    "        postfix_metrics['Best Val Loss'] = f\"{best_val_metric_for_loss:.4f}\"\n",
    "    epoch_pbar.set_postfix(postfix_metrics)\n",
    "\n",
    "    # --- Save model at the end of EACH epoch ---\n",
    "    epoch_model_filename = f\"{base_filename_prefix}_epoch_{epoch+1:02d}.pth\" # e.g., epoch_01.pth\n",
    "    epoch_model_path = os.path.join(MODEL_SAVE_DIR, epoch_model_filename)\n",
    "    try:\n",
    "        save_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_map': val_map_50_95,\n",
    "            'num_classes': NUM_CLASSES,\n",
    "            'category_map': category_map\n",
    "        }\n",
    "        if lr_scheduler is not None:\n",
    "            save_dict['lr_scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "\n",
    "        torch.save(save_dict, epoch_model_path)\n",
    "        # tqdm.write(f\"Epoch {epoch+1}: Model saved to {epoch_model_path}\") # Optional: can be verbose\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Epoch {epoch+1}: Error saving model to {epoch_model_path}: {e}\")\n",
    "\n",
    "\n",
    "    # --- Save the best model (based on chosen metric) ---\n",
    "    current_eval_metric = val_map_50_95 if use_map_for_best else avg_val_loss\n",
    "    is_better = False\n",
    "    if use_map_for_best:\n",
    "        if current_eval_metric > best_val_map:\n",
    "            is_better = True\n",
    "            best_val_map = current_eval_metric\n",
    "            best_val_metric_for_loss = avg_val_loss # Also track loss when mAP is best\n",
    "    else:\n",
    "        if current_eval_metric < best_val_metric_for_loss:\n",
    "            is_better = True\n",
    "            best_val_metric_for_loss = current_eval_metric\n",
    "    if is_better:\n",
    "        tqdm.write(f\"Epoch {epoch+1}: Saving new best model to {best_model_path}\") # Optional\n",
    "        try:\n",
    "            best_save_dict = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_metric_for_loss, # This is avg_val_loss from the best epoch\n",
    "                'best_val_map': best_val_map,           # This is val_map_50_95 from the best epoch\n",
    "                'num_classes': NUM_CLASSES,\n",
    "                'category_map': category_map\n",
    "            }\n",
    "            if lr_scheduler is not None:\n",
    "                best_save_dict['lr_scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "\n",
    "            torch.save(best_save_dict, best_model_path)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Epoch {epoch+1}: Error saving best model: {e}\")\n",
    "    else:\n",
    "       tqdm.write(f\"Epoch {epoch+1}: Validation metric did not improve.\") # Optional\n",
    "\n",
    "epoch_pbar.close()\n",
    "\n",
    "total_training_time = time.time() - start_training_time\n",
    "print(f\"\\n--- Training Finished ---\")\n",
    "print(f\"Total Training Time: {str(datetime.timedelta(seconds=int(total_training_time)))}\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    try:\n",
    "        best_checkpoint = torch.load(best_model_path, map_location='cpu') # map_location if not loading on same device\n",
    "        best_epoch_saved = best_checkpoint.get('epoch', -1) + 1 # epoch is 0-indexed\n",
    "        saved_val_loss = best_checkpoint.get('best_val_loss', float('nan'))\n",
    "        saved_val_map = best_checkpoint.get('best_val_map', float('nan'))\n",
    "        print(f\"Best model saved from epoch {best_epoch_saved} at '{best_model_path}'\")\n",
    "        print(f\"  with Validation Loss: {saved_val_loss:.4f} and mAP@.5-.95: {saved_val_map:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading best model details from {best_model_path}: {e}\")\n",
    "        print(f\"Tracked during training - Best Validation Loss: {best_val_metric_for_loss:.4f}, Best mAP@.5-.95: {best_val_map:.4f}\")\n",
    "else:\n",
    "    print(\"No best model was saved (or path is incorrect).\")\n",
    "    print(f\"Metrics at end of training - Best Val Loss (if loss was criteria): {best_val_metric_for_loss:.4f}, Best Val mAP (if mAP was criteria): {best_val_map:.4f}\")\n"
   ],
   "id": "ba262b423278f5aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Overall Training Progress:   0%|          | 0/25 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa5cf81123754dbb9656a45998b84047"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/810 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c8ad7fce06f4eb6b27d8ddc2a3077cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # For epochs range if needed\n",
    "\n",
    "# Ensure the history dictionary is populated and has data\n",
    "if history['val_loss'] and history['val_map']:\n",
    "    epochs_range = range(1, len(history['val_loss']) + 1)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot Validation Loss on the first y-axis\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Validation Loss', color=color)\n",
    "    ax1.plot(epochs_range, history['val_loss'], color=color, marker='o', linestyle='-', label='Validation Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Create a second y-axis that shares the same x-axis for Validation mAP\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Validation mAP@.5-.95', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(epochs_range, history['val_map'], color=color, marker='s', linestyle='--', label='Validation mAP@.5-.95')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title('Validation Loss and mAP Over Epochs')\n",
    "\n",
    "    # Add legends\n",
    "    # To combine legends from both axes:\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='best') # loc='best' tries to find the best spot\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "\n",
    "elif history['val_loss']: # Only validation loss is available\n",
    "    epochs_range = range(1, len(history['val_loss']) + 1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_range, history['val_loss'], marker='o', linestyle='-', label='Validation Loss')\n",
    "    plt.title('Validation Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"No validation data in history to plot.\")\n",
    "\n",
    "# You can also plot training loss if desired\n",
    "if history['train_loss']:\n",
    "    epochs_range_train = range(1, len(history['train_loss']) + 1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_range_train, history['train_loss'], marker='x', linestyle='-', label='Training Loss', color='tab:green')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "4c97035ad4bbcfba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Evaluating Best Model on Test Set ---\")\n",
    "best_model_path = os.path.join(MODEL_SAVE_DIR, \"frcnn_freeze_backbone_epoch_04.pth\")\n",
    "\n",
    "if not os.path.exists(best_model_path):\n",
    "    print(\"Error: Best model file not found. Skipping test evaluation.\")\n",
    "else:\n",
    "    print(f\"Loading best model from: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
    "\n",
    "    # Ensure NUM_CLASSES matches the saved model\n",
    "    saved_num_classes = checkpoint.get('num_classes', NUM_CLASSES) # Default to config if not saved\n",
    "    if saved_num_classes != NUM_CLASSES:\n",
    "        print(f\"Warning: NUM_CLASSES in config ({NUM_CLASSES}) differs from saved model ({saved_num_classes}). Using saved value.\")\n",
    "        NUM_CLASSES = saved_num_classes\n",
    "\n",
    "    eval_model, _ = get_faster_rcnn_model(NUM_CLASSES) # Create model structure\n",
    "    eval_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    eval_model.to(DEVICE)\n",
    "    eval_model.eval() # Set to evaluation mode\n",
    "\n",
    "    print(\"Best model loaded successfully.\")\n",
    "\n",
    "    # --- Evaluate on Test Set ---\n",
    "    # We use the same evaluate function but pass the test data loader\n",
    "    # It will build the ground truth COCO object for the test set this time\n",
    "    if COCO is None or COCOeval is None:\n",
    "         print(\"pycocotools not available. Skipping COCO evaluation on test set.\")\n",
    "    else:\n",
    "         print(\"\\nRunning evaluation on the Test Set...\")\n",
    "         _, test_coco_stats = evaluate(eval_model, data_loader_test, DEVICE)\n",
    "\n",
    "         if test_coco_stats:\n",
    "             print(\"\\n--- Test Set Evaluation Summary (COCO Metrics) ---\")\n",
    "             print(f\"mAP @ IoU=0.50:0.95 | area=all | maxDets=100: {test_coco_stats[0]:.4f}\")\n",
    "             print(f\"mAP @ IoU=0.50      | area=all | maxDets=100: {test_coco_stats[1]:.4f}\")\n",
    "             print(f\"mAP @ IoU=0.75      | area=all | maxDets=100: {test_coco_stats[2]:.4f}\")\n",
    "             print(f\"mAP @ IoU=0.50:0.95 | area=small | maxDets=100: {test_coco_stats[3]:.4f}\")\n",
    "             print(f\"mAP @ IoU=0.50:0.95 | area=medium| maxDets=100: {test_coco_stats[4]:.4f}\")\n",
    "             print(f\"mAP @ IoU=0.50:0.95 | area=large | maxDets=100: {test_coco_stats[5]:.4f}\")\n",
    "             # ... and so on for AR stats if needed (indices 6-11)\n",
    "         else:\n",
    "             print(\"COCO evaluation on test set could not be completed.\")"
   ],
   "id": "d2502bce6f0fa1f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_image_with_boxes(image_tensor, gt_boxes=None, pred_boxes=None, pred_scores=None, pred_labels=None, gt_labels=None, category_map=None, score_threshold=0.5, title=\"Image\"):\n",
    "    \"\"\"Helper function to plot image with bounding boxes.\"\"\"\n",
    "    if image_tensor.is_cuda:\n",
    "        image_tensor = image_tensor.cpu()\n",
    "    if gt_boxes is not None and gt_boxes.is_cuda: gt_boxes = gt_boxes.cpu()\n",
    "    if pred_boxes is not None and pred_boxes.is_cuda: pred_boxes = pred_boxes.cpu()\n",
    "    if pred_scores is not None and pred_scores.is_cuda: pred_scores = pred_scores.cpu()\n",
    "    if pred_labels is not None and pred_labels.is_cuda: pred_labels = pred_labels.cpu()\n",
    "    if gt_labels is not None and gt_labels.is_cuda: gt_labels = gt_labels.cpu()\n",
    "\n",
    "\n",
    "    # Convert tensor image back to PIL format for plotting\n",
    "    img = T.ToPILImage()(image_tensor)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    plt.title(title)\n",
    "\n",
    "    # Reverse category map {id: name}\n",
    "    id_to_name = {v: k for k, v in category_map.items()} if category_map else {}\n",
    "\n",
    "    # Plot Ground Truth boxes (Green)\n",
    "    if gt_boxes is not None and len(gt_boxes) > 0:\n",
    "        for i, box in enumerate(gt_boxes):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='g', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            label_id = gt_labels[i].item() if gt_labels is not None else 0\n",
    "            label_name = id_to_name.get(label_id, f'ID:{label_id}')\n",
    "            plt.text(xmin, ymin - 5, f'GT: {label_name}', color='g', fontsize=9, bbox=dict(facecolor='white', alpha=0.5, pad=0))\n",
    "\n",
    "    # Plot Predicted boxes (Red)\n",
    "    if pred_boxes is not None and len(pred_boxes) > 0:\n",
    "        for i, box in enumerate(pred_boxes):\n",
    "            score = pred_scores[i].item() if pred_scores is not None else 1.0\n",
    "            if score >= score_threshold:\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                label_id = pred_labels[i].item() if pred_labels is not None else 0\n",
    "                label_name = id_to_name.get(label_id, f'ID:{label_id}')\n",
    "                plt.text(xmax, ymin - 5, f'Pred: {label_name} ({score:.2f})', color='r', fontsize=9, ha='right', bbox=dict(facecolor='white', alpha=0.5, pad=0))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ],
   "id": "e749a491cb2af066",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_image_with_boxes(\n",
    "    image_tensor,\n",
    "    gt_boxes=None,\n",
    "    gt_labels=None,\n",
    "    pred_boxes=None,\n",
    "    pred_scores=None,\n",
    "    pred_labels=None,\n",
    "    category_map=None,\n",
    "    score_threshold=0.5,\n",
    "    title=\"\",\n",
    "    ax=None\n",
    "):\n",
    "    if ax is None:\n",
    "        fig, current_ax = plt.subplots(1, figsize=(12, 9))\n",
    "    else:\n",
    "        current_ax = ax\n",
    "        fig = current_ax.figure\n",
    "\n",
    "    img_display = image_tensor.cpu().permute(1, 2, 0).numpy()\n",
    "    if img_display.max() <= 1.0:\n",
    "        img_display = (img_display * 255).astype(np.uint8)\n",
    "    else:\n",
    "        img_display = img_display.astype(np.uint8)\n",
    "    if img_display.shape[2] == 1:\n",
    "        img_display = cv2.cvtColor(img_display, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    current_ax.imshow(img_display)\n",
    "    current_ax.axis('off')\n",
    "\n",
    "    def draw_boxes_on_ax(boxes_tensor, labels_tensor, scores_tensor, color, box_type_name):\n",
    "        if boxes_tensor is not None and len(boxes_tensor) > 0:\n",
    "            boxes = boxes_tensor.cpu().numpy()\n",
    "            labels = labels_tensor.cpu().numpy()\n",
    "            if scores_tensor is not None:\n",
    "                scores = scores_tensor.cpu().numpy()\n",
    "\n",
    "            for i in range(len(boxes)):\n",
    "                if scores_tensor is not None and scores[i] < score_threshold and box_type_name == \"Prediction\":\n",
    "                    continue\n",
    "                box = boxes[i]\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                         linewidth=2, edgecolor=color, facecolor='none')\n",
    "                current_ax.add_patch(rect)\n",
    "                label_id = labels[i]\n",
    "                label_name = category_map.get(label_id, f'Label {label_id}')\n",
    "                text = f\"{label_name}\"\n",
    "                if scores_tensor is not None:\n",
    "                    text += f\": {scores[i]:.2f}\"\n",
    "                current_ax.text(xmin, ymin - 5, text, color='white',\n",
    "                                bbox=dict(facecolor=color, alpha=0.7, pad=0.3, edgecolor='none'))\n",
    "\n",
    "    if gt_boxes is not None and gt_labels is not None:\n",
    "        draw_boxes_on_ax(gt_boxes, gt_labels, None, 'lime', \"GT\")\n",
    "    if pred_boxes is not None and pred_labels is not None and pred_scores is not None:\n",
    "        draw_boxes_on_ax(pred_boxes, pred_labels, pred_scores, 'red', \"Prediction\")\n",
    "    if title:\n",
    "        current_ax.set_title(title)\n",
    "    return fig"
   ],
   "id": "5f9b8173cf52f35d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "# --- Main Real-time Display Logic ---\n",
    "if 'eval_model' in locals():\n",
    "    print(\"\\nVisualizing Test Set predictions in real-time (Press 'q' to quit)...\")\n",
    "    num_samples_to_show = 5000 # Will run until 'q' is pressed or data ends\n",
    "    eval_model.eval()\n",
    "\n",
    "    # For consistent window size from Matplotlib plot\n",
    "    fig_width_inches = 12\n",
    "    fig_height_inches = 9\n",
    "\n",
    "    processed_frames_count = 0\n",
    "    cv2_window_name = \"Real-time Predictions\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(data_loader_test):\n",
    "            if processed_frames_count >= num_samples_to_show:\n",
    "                print(f\"Reached {num_samples_to_show} samples. Stopping.\")\n",
    "                break\n",
    "\n",
    "            images_device = list(img.to(DEVICE) for img in images)\n",
    "            outputs = eval_model(images_device)\n",
    "\n",
    "            # Plot the first image of the batch\n",
    "            img_idx = 0\n",
    "            if img_idx >= len(images):\n",
    "                continue\n",
    "\n",
    "            image_tensor = images[img_idx]\n",
    "            target = targets[img_idx]\n",
    "            output = outputs[img_idx]\n",
    "\n",
    "            # 1. Create a Matplotlib figure and axis for this frame\n",
    "            fig, ax = plt.subplots(figsize=(fig_width_inches, fig_height_inches))\n",
    "\n",
    "            # 2. Use the plotting function, passing the axis\n",
    "            plot_image_with_boxes(\n",
    "                image_tensor,\n",
    "                gt_boxes=target['boxes'],\n",
    "                gt_labels=target['labels'],\n",
    "                pred_boxes=output['boxes'].cpu(),\n",
    "                pred_scores=output['scores'].cpu(),\n",
    "                pred_labels=output['labels'].cpu(),\n",
    "                category_map=category_map,\n",
    "                score_threshold=0.6,\n",
    "                title=f\"Frame {processed_frames_count} (Batch {i}, Img {img_idx})\",\n",
    "                ax=ax\n",
    "            )\n",
    "\n",
    "            # 3. Render the Matplotlib figure to a NumPy array\n",
    "            fig.canvas.draw()\n",
    "            img_buf = fig.canvas.buffer_rgba()\n",
    "            frame_rgba = np.asarray(img_buf)\n",
    "            frame_bgr = cv2.cvtColor(frame_rgba, cv2.COLOR_RGBA2BGR)\n",
    "\n",
    "            # 4. Close the Matplotlib figure to free memory\n",
    "            plt.close(fig)\n",
    "\n",
    "            # 5. Display the frame in an OpenCV window\n",
    "            cv2.imshow(cv2_window_name, frame_bgr)\n",
    "\n",
    "            processed_frames_count += 1\n",
    "\n",
    "            # 6. Wait for a key press (e.g., 30ms delay for ~33 FPS display, adjust as needed)\n",
    "            #    If 'q' is pressed, break the loop.\n",
    "            #    cv2.waitKey(0) would wait indefinitely for a key.\n",
    "            #    cv2.waitKey(1) is often used for maximum speed.\n",
    "            key = cv2.waitKey(30) & 0xFF # The 0xFF mask is for 64-bit systems\n",
    "            if key == ord('q'):\n",
    "                print(\"'q' pressed, exiting visualization.\")\n",
    "                break\n",
    "            elif key == ord('p'): # Add a pause functionality\n",
    "                print(\"Paused. Press any key to continue (except 'q' to quit).\")\n",
    "                while True:\n",
    "                    pause_key = cv2.waitKey(0) & 0xFF\n",
    "                    if pause_key == ord('q'):\n",
    "                        key = ord('q') # Propagate quit command\n",
    "                        break\n",
    "                    elif pause_key != ord('p'): # Any other key to unpause\n",
    "                        break\n",
    "                if key == ord('q'):\n",
    "                    print(\"'q' pressed during pause, exiting visualization.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "    # 7. Clean up OpenCV windows\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"\\nReal-time visualization finished. Processed {processed_frames_count} frames.\")\n",
    "\n",
    "else:\n",
    "    print(\"Best model not loaded. Skipping real-time visualization.\")"
   ],
   "id": "f697e97d03705261",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
