{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import pickle # For caching splits"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.ops import box_iou\n",
    "import torchvision.transforms.v2 as T # Use new v2 transforms API\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm.notebook import tqdm # Use notebook version for better display"
   ],
   "id": "d8f11eaed1df2baa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    from pycocotools.coco import COCO\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "    print(\"pycocotools found.\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: pycocotools not found. Evaluation will fail.\")\n",
    "    print(\"Install it: pip install pycocotools\")\n",
    "    COCO = None\n",
    "    COCOeval = None\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # Filter UserWarnings if they become noisy"
   ],
   "id": "4b4750271d43bd47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BASE_DATA_DIR = \"data\"\n",
    "FRAME_BASE_DIR = os.path.join(BASE_DATA_DIR, \"frame\")\n",
    "ANNOTATION_BASE_DIR = os.path.join(BASE_DATA_DIR, \"annotation\", \"coco\")\n",
    "MODEL_SAVE_DIR = \"models\"\n",
    "CACHE_DIR = \"cache\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ],
   "id": "858162262aa98498",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Data Splitting & Caching\n",
    "VIDEO_IDS = [f\"{i:02d}\" for i in range(1, 59)] # Assuming 01 to 58\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "# TEST_RATIO is implicitly 1.0 - TRAIN_RATIO - VAL_RATIO\n",
    "SPLIT_CACHE_FILE = os.path.join(CACHE_DIR, \"data_splits.pkl\")\n",
    "FORCE_REGENERATE_SPLITS = False # Set to True to recreate splits, otherwise load from cache if exists"
   ],
   "id": "9e64555912985596",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model & Training Hyperparameters\n",
    "NUM_CLASSES = 1 + 1 # Number of foreground classes + 1 background (UPDATE THIS based on your actual classes!)\n",
    "BATCH_SIZE = 6      # Adjust based on GPU memory\n",
    "NUM_EPOCHS = 15     # Number of training epochs\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0005\n",
    "LR_STEP_SIZE = 5    # Decrease LR every N epochs\n",
    "LR_GAMMA = 0.1      # Factor to decrease LR by\n",
    "NUM_WORKERS = 0     # DataLoader workers (adjust based on CPU cores)\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Number of object classes (excluding background): {NUM_CLASSES - 1}\")"
   ],
   "id": "8f2a5241b8006c39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if base directories exist\n",
    "if not os.path.isdir(FRAME_BASE_DIR) or not os.path.isdir(ANNOTATION_BASE_DIR):\n",
    "    raise FileNotFoundError(\"Frame or Annotation directory not found. Please check paths.\")"
   ],
   "id": "16ddc99911df7327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_coco_metadata(annotation_dir, video_ids):\n",
    "    \"\"\"Loads image info and frame counts from multiple COCO JSON files.\"\"\"\n",
    "    all_image_info = [] # List of dicts: {'image_id': id, 'coco_file': path, 'img_info': {...}}\n",
    "    video_frame_counts = {}\n",
    "    categories = {} # {id: name}\n",
    "    cat_name_to_id = {} # {name: id} - Ensure consistency across files\n",
    "\n",
    "    print(f\"Loading metadata from {len(video_ids)} COCO files...\")\n",
    "    for video_id in tqdm(video_ids, desc=\"Loading JSONs\"):\n",
    "        coco_file = os.path.join(annotation_dir, f\"{video_id}.json\")\n",
    "        if not os.path.exists(coco_file):\n",
    "            print(f\"Warning: Annotation file not found: {coco_file}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            with open(coco_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            video_frame_counts[video_id] = len(data.get('images', []))\n",
    "\n",
    "            # Process categories - ensure consistency\n",
    "            if 'categories' in data:\n",
    "                for cat in data['categories']:\n",
    "                    if cat['id'] not in categories:\n",
    "                        categories[cat['id']] = cat['name']\n",
    "                        if cat['name'] not in cat_name_to_id:\n",
    "                             cat_name_to_id[cat['name']] = cat['id']\n",
    "                        elif cat_name_to_id[cat['name']] != cat['id']:\n",
    "                             print(f\"Warning: Category ID mismatch for '{cat['name']}' in {coco_file}. Using first encountered ID {cat_name_to_id[cat['name']]}.\")\n",
    "                    elif categories[cat['id']] != cat['name']:\n",
    "                         print(f\"Warning: Category Name mismatch for ID {cat['id']}' in {coco_file} ('{cat['name']}' vs '{categories[cat['id']]}'). Keeping first name.\")\n",
    "\n",
    "            # Store image references\n",
    "            for img in data.get('images', []):\n",
    "                # Ensure image id is unique globally if merging later, but here it's fine per file\n",
    "                # We store the original image ID from the JSON and the file it came from\n",
    "                all_image_info.append({\n",
    "                    'image_id_in_file': img['id'], # ID within its original JSON\n",
    "                    'coco_file': coco_file,\n",
    "                    'img_info': img # Store the whole image dict\n",
    "                })\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Error decoding JSON file: {coco_file}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing file {coco_file}: {e}. Skipping.\")\n",
    "\n",
    "    print(f\"Total images found across all files: {len(all_image_info)}\")\n",
    "    if not categories:\n",
    "         raise ValueError(\"No categories found in any annotation file. Cannot proceed.\")\n",
    "\n",
    "    # Create a sorted list of category names and a mapping to contiguous IDs (1-based)\n",
    "    sorted_cat_names = sorted(list(cat_name_to_id.keys()))\n",
    "    cat_name_to_contiguous_id = {name: i + 1 for i, name in enumerate(sorted_cat_names)} # 1-based index\n",
    "    print(\"\\nCategories found:\", cat_name_to_contiguous_id)\n",
    "\n",
    "    # Verify NUM_CLASSES matches detected categories\n",
    "    if NUM_CLASSES != (len(cat_name_to_contiguous_id) + 1):\n",
    "        print(f\"\\nWARNING: NUM_CLASSES configured ({NUM_CLASSES}) does not match detected categories ({len(cat_name_to_contiguous_id)} + 1 background).\")\n",
    "        print(\"Please update NUM_CLASSES in the configuration.\")\n",
    "        # Optionally raise error: raise ValueError(\"NUM_CLASSES mismatch\")\n",
    "\n",
    "    return all_image_info, video_frame_counts, cat_name_to_contiguous_id"
   ],
   "id": "63a911570ecf2221",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_image_references, video_frame_counts, category_map = load_coco_metadata(ANNOTATION_BASE_DIR, VIDEO_IDS)",
   "id": "d2ba79d71ef02122",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_balanced_splits(video_ids, video_frame_counts, train_ratio, val_ratio):\n",
    "    \"\"\"Splits video IDs ensuring balanced frame counts.\"\"\"\n",
    "    num_videos = len(video_ids)\n",
    "    num_train = math.floor(train_ratio * num_videos)\n",
    "    num_val = math.floor(val_ratio * num_videos)\n",
    "    num_test = num_videos - num_train - num_val\n",
    "\n",
    "    print(f\"\\nTarget split sizes (by video count): Train={num_train}, Val={num_val}, Test={num_test}\")\n",
    "\n",
    "    # Shuffle videos for randomness\n",
    "    shuffled_video_ids = random.sample(video_ids, len(video_ids))\n",
    "\n",
    "    # Sort shuffled videos by frame count (desc) to help balance assignment\n",
    "    sorted_shuffled_videos = sorted(shuffled_video_ids, key=lambda vid: video_frame_counts.get(vid, 0), reverse=True)\n",
    "\n",
    "    train_vids, val_vids, test_vids = [], [], []\n",
    "    train_frames, val_frames, test_frames = 0, 0, 0\n",
    "\n",
    "    # Assign videos greedily to the split with the fewest frames currently\n",
    "    for video_id in sorted_shuffled_videos:\n",
    "        frames = video_frame_counts.get(video_id, 0)\n",
    "        assigned = False\n",
    "        # Prioritize filling smaller target sets first if counts are equal\n",
    "        split_counts = [\n",
    "            (train_frames, train_vids, num_train),\n",
    "            (val_frames, val_vids, num_val),\n",
    "            (test_frames, test_vids, num_test)\n",
    "        ]\n",
    "        # Sort splits by current frame count, then by remaining capacity (desc)\n",
    "        split_counts.sort(key=lambda x: (x[0], -(x[2] - len(x[1])) ))\n",
    "\n",
    "        for i in range(len(split_counts)):\n",
    "            current_frames, vid_list, target_count = split_counts[i]\n",
    "            if len(vid_list) < target_count:\n",
    "                 vid_list.append(video_id)\n",
    "                 # Update frame counts directly (list references mutable lists)\n",
    "                 if vid_list is train_vids: train_frames += frames\n",
    "                 elif vid_list is val_vids: val_frames += frames\n",
    "                 else: test_frames += frames\n",
    "                 assigned = True\n",
    "                 break\n",
    "\n",
    "        if not assigned: # Should not happen if ratios sum <= 1\n",
    "            print(f\"Warning: Could not assign video {video_id}. Adding to train set.\")\n",
    "            train_vids.append(video_id)\n",
    "            train_frames += frames\n",
    "\n",
    "\n",
    "    print(\"\\nActual split counts:\")\n",
    "    print(f\"  Train: {len(train_vids)} videos, {train_frames} frames\")\n",
    "    print(f\"  Val:   {len(val_vids)} videos, {val_frames} frames\")\n",
    "    print(f\"  Test:  {len(test_vids)} videos, {test_frames} frames\")\n",
    "\n",
    "    return set(train_vids), set(val_vids), set(test_vids)"
   ],
   "id": "922c7a65383f1a49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not FORCE_REGENERATE_SPLITS and os.path.exists(SPLIT_CACHE_FILE):\n",
    "    print(f\"Loading cached splits from {SPLIT_CACHE_FILE}...\")\n",
    "    with open(SPLIT_CACHE_FILE, 'rb') as f:\n",
    "        split_data = pickle.load(f)\n",
    "    train_image_refs = split_data['train']\n",
    "    val_image_refs = split_data['val']\n",
    "    test_image_refs = split_data['test']\n",
    "    category_map = split_data['category_map'] # Load map from cache too\n",
    "    print(f\"Loaded splits: Train={len(train_image_refs)}, Val={len(val_image_refs)}, Test={len(test_image_refs)} images.\")\n",
    "    # Re-verify NUM_CLASSES from cached map\n",
    "    if NUM_CLASSES != (len(category_map) + 1):\n",
    "        print(f\"\\nWARNING: NUM_CLASSES configured ({NUM_CLASSES}) does not match cached category map ({len(category_map)} + 1 background). Using map from cache.\")\n",
    "        NUM_CLASSES = len(category_map) + 1\n",
    "\n",
    "else:\n",
    "    print(\"Generating new data splits...\")\n",
    "    train_vids, val_vids, test_vids = create_balanced_splits(\n",
    "        list(video_frame_counts.keys()), # Only use videos we found annotations for\n",
    "        video_frame_counts,\n",
    "        TRAIN_RATIO,\n",
    "        VAL_RATIO\n",
    "    )\n",
    "\n",
    "    # Filter all_image_references based on video ID splits\n",
    "    train_image_refs, val_image_refs, test_image_refs = [], [], []\n",
    "    for ref in all_image_references:\n",
    "        # Extract video ID from coco_file path\n",
    "        video_id = os.path.splitext(os.path.basename(ref['coco_file']))[0]\n",
    "        if video_id in train_vids:\n",
    "            train_image_refs.append(ref)\n",
    "        elif video_id in val_vids:\n",
    "            val_image_refs.append(ref)\n",
    "        elif video_id in test_vids:\n",
    "            test_image_refs.append(ref)\n",
    "\n",
    "    # Cache the splits\n",
    "    print(f\"\\nCaching splits to {SPLIT_CACHE_FILE}...\")\n",
    "    split_data_to_cache = {\n",
    "        'train': train_image_refs,\n",
    "        'val': val_image_refs,\n",
    "        'test': test_image_refs,\n",
    "        'category_map': category_map, # Save the category map used for this split\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    try:\n",
    "        with open(SPLIT_CACHE_FILE, 'wb') as f:\n",
    "            pickle.dump(split_data_to_cache, f)\n",
    "        print(\"Splits cached successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error caching splits: {e}\")"
   ],
   "id": "f1990de2c414542",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sanity check\n",
    "print(f\"\\nFinal split sizes (images): Train={len(train_image_refs)}, Val={len(val_image_refs)}, Test={len(test_image_refs)}\")\n",
    "total_split = len(train_image_refs) + len(val_image_refs) + len(test_image_refs)\n",
    "print(f\"Total images in splits: {total_split} (Should match total found images if all videos were assigned)\")\n",
    "if total_split != len(all_image_references):\n",
    "    print(\"Warning: Total images in splits don't match initial count. Some video IDs might be missing from splits.\")"
   ],
   "id": "75210654bd9ebba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CocoMultiJsonDataset(Dataset):\n",
    "    def __init__(self, image_references, frame_base_dir, category_map, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_references (list): List of dicts {'image_id_in_file', 'coco_file', 'img_info'}.\n",
    "            frame_base_dir (str): Base directory where frame folders (01, 02...) are located.\n",
    "            category_map (dict): Mapping from category name to contiguous ID (1-based).\n",
    "            transforms (callable, optional): Transformations to apply.\n",
    "        \"\"\"\n",
    "        self.image_refs = image_references\n",
    "        self.frame_base_dir = frame_base_dir\n",
    "        self.transforms = transforms\n",
    "        self.category_map = category_map\n",
    "        self.cat_id_to_contiguous_id = {} # Map original category ID from JSON to contiguous ID\n",
    "\n",
    "        # Pre-load COCO objects or annotations per file for efficiency?\n",
    "        # For moderate number of files, loading on demand might be okay.\n",
    "        # If very slow, consider pre-loading into a dictionary:\n",
    "        self._coco_cache = {} # Cache loaded COCO objects or just annotations\n",
    "\n",
    "    def _get_coco_annotations(self, coco_file_path):\n",
    "        \"\"\"Loads annotations for a specific COCO file, caching results.\"\"\"\n",
    "        if coco_file_path not in self._coco_cache:\n",
    "            try:\n",
    "                with open(coco_file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                # Index annotations by image_id for faster lookup\n",
    "                ann_by_img_id = {}\n",
    "                for ann in data.get('annotations', []):\n",
    "                    img_id = ann['image_id']\n",
    "                    if img_id not in ann_by_img_id:\n",
    "                        ann_by_img_id[img_id] = []\n",
    "                    ann_by_img_id[img_id].append(ann)\n",
    "\n",
    "                 # Map original category IDs to contiguous IDs for this file\n",
    "                local_cat_id_map = {}\n",
    "                for cat in data.get('categories', []):\n",
    "                    cat_name = cat['name']\n",
    "                    if cat_name in self.category_map:\n",
    "                        local_cat_id_map[cat['id']] = self.category_map[cat_name]\n",
    "                    # else: # Should not happen if map is built correctly\n",
    "                    #     print(f\"Warning: Category '{cat_name}' from {coco_file_path} not in global map.\")\n",
    "\n",
    "                self._coco_cache[coco_file_path] = {\n",
    "                    'annotations': ann_by_img_id,\n",
    "                    'cat_id_map': local_cat_id_map\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading or processing COCO file {coco_file_path}: {e}\")\n",
    "                self._coco_cache[coco_file_path] = {'annotations': {}, 'cat_id_map': {}} # Mark as failed\n",
    "\n",
    "        return self._coco_cache[coco_file_path]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_refs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_ref = self.image_refs[idx]\n",
    "        img_metadata = img_ref['img_info']\n",
    "        coco_file = img_ref['coco_file']\n",
    "        img_id_in_file = img_ref['image_id_in_file'] # Use the ID specific to that JSON\n",
    "\n",
    "        # Construct image path (handle potential path separators)\n",
    "        # img_metadata['file_name'] might be like \"03\\\\frame_000000.jpg\" or \"03/frame_000000.jpg\"\n",
    "        relative_img_path = img_metadata['file_name'].replace('\\\\', os.sep).replace('/', os.sep)\n",
    "        img_path = os.path.join(self.frame_base_dir, relative_img_path)\n",
    "\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Error: Image file not found at {img_path}\")\n",
    "             # Return dummy data or raise error, depending on desired behavior\n",
    "             # Returning dummy data to avoid crashing DataLoader completely\n",
    "             w, h = img_metadata.get('width', 64), img_metadata.get('height', 64) # Use metadata size if possible\n",
    "             return T.ToTensor()(Image.new('RGB', (w, h))), {'boxes': torch.empty((0, 4)), 'labels': torch.empty(0, dtype=torch.int64)}\n",
    "\n",
    "\n",
    "        # Load annotations for this image from its corresponding COCO file\n",
    "        coco_data = self._get_coco_annotations(coco_file)\n",
    "        annotations = coco_data['annotations'].get(img_id_in_file, [])\n",
    "        local_cat_id_map = coco_data['cat_id_map']\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for ann in annotations:\n",
    "            # Convert COCO bbox [xmin, ymin, width, height] to [xmin, ymin, xmax, ymax]\n",
    "            xmin = ann['bbox'][0]\n",
    "            ymin = ann['bbox'][1]\n",
    "            xmax = xmin + ann['bbox'][2]\n",
    "            ymax = ymin + ann['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            # Map original category ID to contiguous ID\n",
    "            original_cat_id = ann['category_id']\n",
    "            if original_cat_id in local_cat_id_map:\n",
    "                 labels.append(local_cat_id_map[original_cat_id])\n",
    "            else:\n",
    "                 # Should we skip this annotation or assign a default? Skipping for now.\n",
    "                 # print(f\"Warning: Annotation with unknown category ID {original_cat_id} in image {img_id_in_file} from {coco_file}\")\n",
    "                 boxes.pop() # Remove the corresponding box\n",
    "                 continue # Skip this annotation\n",
    "\n",
    "            areas.append(ann.get('area', ann['bbox'][2] * ann['bbox'][3]))\n",
    "            iscrowd.append(ann.get('iscrowd', 0))\n",
    "\n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.uint8)\n",
    "        image_id = torch.tensor([idx]) # Use the index in the *current dataset split* as the image_id for evaluation\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id # Important for COCO eval\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms is not None:\n",
    "            # The new transforms handle image and target dicts together\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        # Ensure boxes are valid after transforms (e.g., width/height > 0)\n",
    "        if \"boxes\" in target:\n",
    "             boxes = target['boxes']\n",
    "             if boxes.shape[0] > 0:\n",
    "                 # Calculate width and height\n",
    "                 widths = boxes[:, 2] - boxes[:, 0]\n",
    "                 heights = boxes[:, 3] - boxes[:, 1]\n",
    "                 # Keep only boxes with positive width and height\n",
    "                 keep = (widths > 0) & (heights > 0)\n",
    "                 if not torch.all(keep):\n",
    "                     # print(f\"Warning: Found invalid boxes after transform for image index {idx}. Filtering.\")\n",
    "                     target['boxes'] = target['boxes'][keep]\n",
    "                     target['labels'] = target['labels'][keep]\n",
    "                     if 'area' in target: target['area'] = target['area'][keep]\n",
    "                     if 'iscrowd' in target: target['iscrowd'] = target['iscrowd'][keep]\n",
    "\n",
    "\n",
    "        return image, target"
   ],
   "id": "f6a72feb37047dbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "\n",
    "    # --- Add this line ---\n",
    "    # Explicitly convert PIL Image to PyTorch Tensor (dtype=uint8, range [0, 255])\n",
    "    transforms.append(T.PILToTensor())\n",
    "    # --- End of Addition ---\n",
    "\n",
    "    if train:\n",
    "        # Standard augmentation for detection\n",
    "        transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
    "        # Add other augmentations if needed (e.g., color jitter, geometric)\n",
    "        # transforms.append(T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1))\n",
    "\n",
    "    # Convert tensor dtype to float and scale to [0, 1]\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True)) # scale=True divides by 255\n",
    "\n",
    "    # Ensure output is pure tensor (might be slightly redundant after ToDtype, but good practice)\n",
    "    transforms.append(T.ToPureTensor())\n",
    "\n",
    "    return T.Compose(transforms)"
   ],
   "id": "14e04bbea12fee58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_train = CocoMultiJsonDataset(train_image_refs, FRAME_BASE_DIR, category_map, get_transform(train=True))\n",
    "dataset_val = CocoMultiJsonDataset(val_image_refs, FRAME_BASE_DIR, category_map, get_transform(train=False))\n",
    "dataset_test = CocoMultiJsonDataset(test_image_refs, FRAME_BASE_DIR, category_map, get_transform(train=False))\n",
    "\n",
    "print(f\"\\nDataset sizes: Train={len(dataset_train)}, Val={len(dataset_val)}, Test={len(dataset_test)}\")"
   ],
   "id": "673e615e2f9a14d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    # Standard collate function for detection: returns tuple(list of images, list of targets)\n",
    "    return tuple(zip(*batch))"
   ],
   "id": "c6010c981cb4c632",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_loader_train = DataLoader(\n",
    "    dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn, pin_memory=torch.cuda.is_available() # Use pin_memory with GPU\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn, pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "data_loader_test = DataLoader(\n",
    "    dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn, pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created with Batch Size={BATCH_SIZE}, Num Workers={NUM_WORKERS}\")"
   ],
   "id": "48c488267624b527",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_faster_rcnn_model(num_classes):\n",
    "    # Load a model pre-trained on COCO\n",
    "    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Replace the pre-trained head with a new one\n",
    "    # num_classes includes the background class\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model, weights # Return weights to get transforms later if needed"
   ],
   "id": "864b5f14dc185bc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model, model_weights = get_faster_rcnn_model(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(\"Faster R-CNN model loaded and modified for custom classes.\")"
   ],
   "id": "7ca26c822067b24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Alternative: AdamW\n",
    "# optimizer = torch.optim.AdamW(params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ],
   "id": "7d709531e6e95f6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEP_SIZE, gamma=LR_GAMMA)",
   "id": "5faf3d76e99789f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50):\n",
    "    model.train() # Set model to training mode\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = f'Epoch: [{epoch}]'\n",
    "\n",
    "    lr_scheduler_val = None # Placeholder, not step-based here\n",
    "\n",
    "    for i, (images, targets) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        try:\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Reduce losses over all GPUs for logging purposes if using DistributedDataParallel\n",
    "            # (Not needed for single GPU/CPU)\n",
    "            loss_dict_reduced = loss_dict # Assume single device\n",
    "            losses_reduced = losses\n",
    "\n",
    "            loss_value = losses_reduced.item()\n",
    "\n",
    "            if not math.isfinite(loss_value):\n",
    "                print(f\"Loss is {loss_value}, stopping training\")\n",
    "                print(loss_dict_reduced)\n",
    "                # Consider saving state or debugging here\n",
    "                return None # Indicate failure\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "            metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training iteration {i}: {e}\")\n",
    "            # Optionally: Save data that caused the error for debugging\n",
    "            # torch.save({'images': images, 'targets': targets}, 'error_batch.pt')\n",
    "            print(\"Skipping this batch.\")\n",
    "            # Ensure optimizer state doesn't get corrupted if possible\n",
    "            optimizer.zero_grad() # Clear gradients from potential partial backward pass\n",
    "            continue # Skip to next batch\n",
    "\n",
    "    return metric_logger"
   ],
   "id": "edbca86105594ef6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@torch.inference_mode() # More efficient than torch.no_grad() for inference\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = 'Validate:'\n",
    "    coco_gt = None # Ground truth COCO object (built from validation set)\n",
    "    coco_predictions = [] # List to store predictions in COCO format\n",
    "\n",
    "    # Check if pycocotools is available\n",
    "    use_coco_eval = COCO is not None and COCOeval is not None\n",
    "\n",
    "    if use_coco_eval:\n",
    "        # Need to build a temporary COCO ground truth object from our validation dataset\n",
    "        # This is a bit inefficient but necessary for pycocotools\n",
    "        print(\"Building temporary COCO ground truth for validation evaluation...\")\n",
    "        val_gt_data = {'images': [], 'annotations': [], 'categories': []}\n",
    "        # Use the actual category map\n",
    "        for name, id_val in category_map.items():\n",
    "             val_gt_data['categories'].append({'id': id_val, 'name': name, 'supercategory': 'object'})\n",
    "\n",
    "        ann_id_counter = 1\n",
    "        img_id_map = {} # Map dataset index to a unique image id for COCO eval\n",
    "        current_img_id = 0\n",
    "\n",
    "        for i, (_, targets_batch) in enumerate(tqdm(data_loader, desc=\"Building Val GT\")):\n",
    "             for targets in targets_batch: # Process each target dict in the batch\n",
    "                 dataset_img_idx = targets['image_id'].item() # Get original dataset index\n",
    "                 if dataset_img_idx not in img_id_map:\n",
    "                     img_id_map[dataset_img_idx] = current_img_id\n",
    "                     # Find original image info (less efficient this way, could store in dataset item)\n",
    "                     original_ref = data_loader.dataset.image_refs[dataset_img_idx]\n",
    "                     img_info = original_ref['img_info']\n",
    "                     val_gt_data['images'].append({\n",
    "                         'id': current_img_id,\n",
    "                         'width': img_info.get('width', 0), # Get dimensions if available\n",
    "                         'height': img_info.get('height', 0),\n",
    "                         'file_name': img_info.get('file_name', f'img_{current_img_id}')\n",
    "                     })\n",
    "                     current_img_id += 1\n",
    "\n",
    "                 coco_img_id = img_id_map[dataset_img_idx]\n",
    "\n",
    "                 boxes = targets['boxes'].cpu().numpy()\n",
    "                 labels = targets['labels'].cpu().numpy()\n",
    "                 areas = targets.get('area', torch.zeros(len(boxes))).cpu().numpy() # Handle missing area\n",
    "\n",
    "                 for j in range(boxes.shape[0]):\n",
    "                     bbox = boxes[j]\n",
    "                     # Convert [xmin, ymin, xmax, ymax] back to [xmin, ymin, width, height]\n",
    "                     coco_bbox = [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]]\n",
    "                     val_gt_data['annotations'].append({\n",
    "                         'id': ann_id_counter,\n",
    "                         'image_id': coco_img_id,\n",
    "                         'category_id': labels[j],\n",
    "                         'bbox': coco_bbox,\n",
    "                         'area': areas[j],\n",
    "                         'iscrowd': targets['iscrowd'][j].cpu().item(),\n",
    "                     })\n",
    "                     ann_id_counter += 1\n",
    "\n",
    "        # Create COCO object from the built ground truth\n",
    "        if val_gt_data['annotations']:\n",
    "            coco_gt = COCO()\n",
    "            coco_gt.dataset = val_gt_data\n",
    "            coco_gt.createIndex()\n",
    "            print(\"Validation ground truth COCO object created.\")\n",
    "        else:\n",
    "            print(\"Warning: No ground truth annotations found for validation set. COCO evaluation will be skipped.\")\n",
    "            use_coco_eval = False\n",
    "\n",
    "\n",
    "    # --- Actual Evaluation Loop ---\n",
    "    val_loss_total = 0.0\n",
    "    val_batches = 0\n",
    "    print(\"\\nRunning validation inference...\")\n",
    "    for images, targets in metric_logger.log_every(data_loader, 50, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Calculate validation loss (optional but good practice)\n",
    "        # Need torch.no_grad() or torch.inference_mode() if model is not already in eval mode\n",
    "        # But if model IS in eval mode, it doesn't return losses by default\n",
    "        # Solution: Temporarily switch to train mode WITH inference_mode context\n",
    "        model.train() # Temp switch to get losses\n",
    "        with torch.inference_mode(): # Still disable gradients\n",
    "            loss_dict = model(images, targets)\n",
    "        model.eval() # Switch back to eval for predictions\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        val_loss_total += losses.item()\n",
    "        val_batches += 1\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        outputs = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in outputs]\n",
    "\n",
    "        # Format predictions for COCOeval\n",
    "        if use_coco_eval and coco_gt:\n",
    "            # Map dataset index back to the temporary COCO image ID\n",
    "            res = []\n",
    "            for i, output in enumerate(outputs):\n",
    "                original_dataset_idx = targets[i]['image_id'].item()\n",
    "                if original_dataset_idx in img_id_map:\n",
    "                     coco_image_id = img_id_map[original_dataset_idx]\n",
    "                     for box, label, score in zip(output['boxes'], output['labels'], output['scores']):\n",
    "                         if score > 0.05: # Confidence threshold for evaluation\n",
    "                             # Convert [xmin, ymin, xmax, ymax] to [xmin, ymin, width, height]\n",
    "                             bbox = box.tolist()\n",
    "                             coco_bbox = [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]]\n",
    "                             res.append({\n",
    "                                 \"image_id\": coco_image_id,\n",
    "                                 \"category_id\": label.item(),\n",
    "                                 \"bbox\": coco_bbox,\n",
    "                                 \"score\": score.item(),\n",
    "                             })\n",
    "            coco_predictions.extend(res)\n",
    "\n",
    "        # Log validation loss if calculated\n",
    "        metric_logger.update(val_loss=losses)\n",
    "\n",
    "    # --- COCO Evaluation ---\n",
    "    eval_summary = None\n",
    "    if use_coco_eval and coco_gt and coco_predictions:\n",
    "        print(\"\\nRunning COCO evaluation on validation predictions...\")\n",
    "        try:\n",
    "            coco_dt = coco_gt.loadRes(coco_predictions)\n",
    "            coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "            coco_eval.evaluate()\n",
    "            coco_eval.accumulate()\n",
    "            coco_eval.summarize()\n",
    "            eval_summary = coco_eval.stats # Store summary stats (mAP etc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during COCO evaluation: {e}\")\n",
    "            eval_summary = None\n",
    "    elif use_coco_eval:\n",
    "         print(\"Skipping COCO evaluation: Ground truth or predictions missing.\")\n",
    "\n",
    "\n",
    "    # Gather all stats from metric_logger\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print('Validation Result: {}'.format(metric_logger))\n",
    "\n",
    "    avg_val_loss = val_loss_total / val_batches if val_batches > 0 else float('inf')\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return avg_val_loss, eval_summary # Return loss and coco stats"
   ],
   "id": "71e6019cafffc4fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import collections\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = collections.deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        # Placeholder for distributed training, does nothing here\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count if self.count > 0 else float('nan')\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.count == 0: return \"N/A\" # Handle empty case\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = collections.defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            if not isinstance(v, (float, int)):\n",
    "                raise TypeError(\n",
    "                    \"This logger accepts only int or float values but received {} of type {}\".format(v, type(v))\n",
    "                )\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\"{}: {}\".format(name, str(meter)))\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n"
   ],
   "id": "7381b099cf6b270",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Starting Training ---\")\n",
    "start_training_time = time.time()\n",
    "best_val_metric = float('inf') # Lower loss is better\n",
    "best_val_map = 0.0 # Higher mAP is better (if using COCO eval)\n",
    "best_model_path = os.path.join(MODEL_SAVE_DIR, \"fasterrcnn_best.pth\")\n",
    "use_map_for_best = COCO is not None # Use mAP if pycocotools is available\n",
    "\n",
    "# Store history\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_map': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{NUM_EPOCHS} =====\")\n",
    "\n",
    "    # Train for one epoch\n",
    "    train_logger = train_one_epoch(model, optimizer, data_loader_train, DEVICE, epoch)\n",
    "    if train_logger is None: # Check if training failed\n",
    "         print(\"Stopping training due to high loss.\")\n",
    "         break\n",
    "\n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    avg_val_loss, coco_stats = evaluate(model, data_loader_val, DEVICE)\n",
    "\n",
    "    # Store history (use global averages from loggers)\n",
    "    history['train_loss'].append(train_logger.meters['loss'].global_avg)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    val_map_50_95 = coco_stats[0] if coco_stats is not None else 0.0 # mAP @ IoU=0.50:0.95\n",
    "    history['val_map'].append(val_map_50_95)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary: Train Loss={train_logger.meters['loss'].global_avg:.4f}, Val Loss={avg_val_loss:.4f}, Val mAP@.5-.95={val_map_50_95:.4f}\")\n",
    "\n",
    "    # --- Save the best model ---\n",
    "    current_metric = val_map_50_95 if use_map_for_best else avg_val_loss\n",
    "    is_better = (current_metric > best_val_map) if use_map_for_best else (current_metric < best_val_metric)\n",
    "\n",
    "    if is_better:\n",
    "        if use_map_for_best:\n",
    "            print(f\"Validation mAP improved ({best_val_map:.4f} --> {current_metric:.4f}). Saving model...\")\n",
    "            best_val_map = current_metric\n",
    "            best_val_metric = avg_val_loss # Still track best loss when map improves\n",
    "        else:\n",
    "            print(f\"Validation loss improved ({best_val_metric:.4f} --> {current_metric:.4f}). Saving model...\")\n",
    "            best_val_metric = current_metric\n",
    "\n",
    "        try:\n",
    "            # Save model state dictionary and other useful info\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_metric,\n",
    "                'best_val_map': best_val_map,\n",
    "                'num_classes': NUM_CLASSES,\n",
    "                'category_map': category_map # Save category map with model\n",
    "            }, best_model_path)\n",
    "            print(f\"Best model saved to {best_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    else:\n",
    "         print(\"Validation metric did not improve.\")\n",
    "\n",
    "\n",
    "total_training_time = time.time() - start_training_time\n",
    "print(f\"\\n--- Training Finished ---\")\n",
    "print(f\"Total Training Time: {str(datetime.timedelta(seconds=int(total_training_time)))}\")\n",
    "print(f\"Best model saved at epoch {torch.load(best_model_path)['epoch']+1 if os.path.exists(best_model_path) else 'N/A'} with Validation Loss: {best_val_metric:.4f} and mAP: {best_val_map:.4f}\")"
   ],
   "id": "ba262b423278f5aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Evaluating Best Model on Test Set ---\")\n",
    "best_model_path = os.path.join(MODEL_SAVE_DIR, \"fasterrcnn_best.pth\")\n",
    "\n",
    "if not os.path.exists(best_model_path):\n",
    "    print(\"Error: Best model file not found. Skipping test evaluation.\")\n",
    "else:\n",
    "    print(f\"Loading best model from: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
    "\n",
    "    # Ensure NUM_CLASSES matches the saved model\n",
    "    saved_num_classes = checkpoint.get('num_classes', NUM_CLASSES) # Default to config if not saved\n",
    "    if saved_num_classes != NUM_CLASSES:\n",
    "        print(f\"Warning: NUM_CLASSES in config ({NUM_CLASSES}) differs from saved model ({saved_num_classes}). Using saved value.\")\n",
    "        NUM_CLASSES = saved_num_classes\n",
    "\n",
    "    eval_model, _ = get_faster_rcnn_model(NUM_CLASSES) # Create model structure\n",
    "    eval_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    eval_model.to(DEVICE)\n",
    "    eval_model.eval() # Set to evaluation mode\n",
    "\n",
    "    print(\"Best model loaded successfully.\")\n",
    "\n",
    "    # --- Evaluate on Test Set ---\n",
    "    # We use the same evaluate function but pass the test data loader\n",
    "    # It will build the ground truth COCO object for the test set this time\n",
    "    if COCO is None or COCOeval is None:\n",
    "         print(\"pycocotools not available. Skipping COCO evaluation on test set.\")\n",
    "    else:\n",
    "         print(\"\\nRunning evaluation on the Test Set...\")\n",
    "         _, test_coco_stats = evaluate(eval_model, data_loader_test, DEVICE)\n",
    "\n",
    "         if test_coco_stats:\n",
    "             print(\"\\n--- Test Set Evaluation Summary (COCO Metrics) ---\")\n",
    "             print(f\"mAP @ IoU=0.50:0.95 | area=all | maxDets=100: {test_coco_stats[0]:.4f}\")\n",
    "             print(f\"mAP @ IoU=0.50      | area=all | maxDets=100: {test_coco_stats[1]:.4f}\")\n",
    "             print(f\"mAP @ IoU=0.75      | area=all | maxDets=100: {test_coco_stats[2]:.4f}\")\n",
    "             print(f\"mAP @ IoU=0.50:0.95 | area=small | maxDets=100: {test_coco_stats[3]:.4f}\")\n",
    "             print(f\"mAP @ IoU=0.50:0.95 | area=medium| maxDets=100: {test_coco_stats[4]:.4f}\")\n",
    "             print(f\"mAP @ IoU=0.50:0.95 | area=large | maxDets=100: {test_coco_stats[5]:.4f}\")\n",
    "             # ... and so on for AR stats if needed (indices 6-11)\n",
    "         else:\n",
    "             print(\"COCO evaluation on test set could not be completed.\")"
   ],
   "id": "d2502bce6f0fa1f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_image_with_boxes(image_tensor, gt_boxes=None, pred_boxes=None, pred_scores=None, pred_labels=None, gt_labels=None, category_map=None, score_threshold=0.5, title=\"Image\"):\n",
    "    \"\"\"Helper function to plot image with bounding boxes.\"\"\"\n",
    "    if image_tensor.is_cuda:\n",
    "        image_tensor = image_tensor.cpu()\n",
    "    if gt_boxes is not None and gt_boxes.is_cuda: gt_boxes = gt_boxes.cpu()\n",
    "    if pred_boxes is not None and pred_boxes.is_cuda: pred_boxes = pred_boxes.cpu()\n",
    "    if pred_scores is not None and pred_scores.is_cuda: pred_scores = pred_scores.cpu()\n",
    "    if pred_labels is not None and pred_labels.is_cuda: pred_labels = pred_labels.cpu()\n",
    "    if gt_labels is not None and gt_labels.is_cuda: gt_labels = gt_labels.cpu()\n",
    "\n",
    "\n",
    "    # Convert tensor image back to PIL format for plotting\n",
    "    img = T.ToPILImage()(image_tensor)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    plt.title(title)\n",
    "\n",
    "    # Reverse category map {id: name}\n",
    "    id_to_name = {v: k for k, v in category_map.items()} if category_map else {}\n",
    "\n",
    "    # Plot Ground Truth boxes (Green)\n",
    "    if gt_boxes is not None and len(gt_boxes) > 0:\n",
    "        for i, box in enumerate(gt_boxes):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='g', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            label_id = gt_labels[i].item() if gt_labels is not None else 0\n",
    "            label_name = id_to_name.get(label_id, f'ID:{label_id}')\n",
    "            plt.text(xmin, ymin - 5, f'GT: {label_name}', color='g', fontsize=9, bbox=dict(facecolor='white', alpha=0.5, pad=0))\n",
    "\n",
    "    # Plot Predicted boxes (Red)\n",
    "    if pred_boxes is not None and len(pred_boxes) > 0:\n",
    "        for i, box in enumerate(pred_boxes):\n",
    "            score = pred_scores[i].item() if pred_scores is not None else 1.0\n",
    "            if score >= score_threshold:\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                label_id = pred_labels[i].item() if pred_labels is not None else 0\n",
    "                label_name = id_to_name.get(label_id, f'ID:{label_id}')\n",
    "                plt.text(xmax, ymin - 5, f'Pred: {label_name} ({score:.2f})', color='r', fontsize=9, ha='right', bbox=dict(facecolor='white', alpha=0.5, pad=0))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ],
   "id": "e749a491cb2af066",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if 'eval_model' in locals(): # Check if model was loaded\n",
    "    print(\"\\nVisualizing some Test Set predictions...\")\n",
    "    num_samples_to_show = 5000\n",
    "    eval_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(data_loader_test):\n",
    "            if i >= num_samples_to_show:\n",
    "                break\n",
    "\n",
    "            images_device = list(img.to(DEVICE) for img in images)\n",
    "            outputs = eval_model(images_device)\n",
    "\n",
    "            # Plot the first image of the batch\n",
    "            img_idx = 0\n",
    "            image_tensor = images[img_idx] # Original tensor before moving to device\n",
    "            target = targets[img_idx]\n",
    "            output = outputs[img_idx]\n",
    "\n",
    "            plot_image_with_boxes(\n",
    "                image_tensor,\n",
    "                gt_boxes=target['boxes'],\n",
    "                gt_labels=target['labels'],\n",
    "                pred_boxes=output['boxes'].cpu(),\n",
    "                pred_scores=output['scores'].cpu(),\n",
    "                pred_labels=output['labels'].cpu(),\n",
    "                category_map=category_map, # Use the loaded/cached map\n",
    "                score_threshold=0.5,\n",
    "                title=f\"Test Image {i*BATCH_SIZE + img_idx}\"\n",
    "            )\n",
    "\n",
    "else:\n",
    "    print(\"Best model not loaded. Skipping visualization.\")"
   ],
   "id": "5f9b8173cf52f35d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
