{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FRCNN + EasyOCR",
   "id": "7d9e527d2186d1ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Dependencies",
   "id": "3974c4696725e43b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:40:14.718258Z",
     "start_time": "2025-05-11T12:40:12.671439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "from models.EasyOCR.utils import AttrDict, AttnLabelConverter, CTCLabelConverter\n",
    "import yaml\n",
    "from torchvision import transforms\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "a4b35a3982e5aff4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "5138ae8d6a9b41a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:40:14.781822Z",
     "start_time": "2025-05-11T12:40:14.766825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Configuration for Detection ---\n",
    "CUSTOM_MODEL_CLASS_NAMES = ['__background__', 'carplate'] # Should match your model's training\n",
    "TARGET_CLASS_NAME = \"carplate\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "BOX_COLOR = (0, 0, 255) # Red for car plates\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "TEXT_BG_COLOR = (0, 0, 0) # Black\n",
    "\n",
    "OCR_BOX_COLOR = (0, 255, 0) # Green for OCR text bounding box (if you draw it)\n",
    "OCR_TEXT_COLOR = (50, 200, 255)  # Light blue/yellow for OCR text\n",
    "OCR_TEXT_BG_COLOR = (0,0,0) # Black background for OCR text\n",
    "\n",
    "try:\n",
    "    TARGET_CLASS_ID = CUSTOM_MODEL_CLASS_NAMES.index(TARGET_CLASS_NAME.lower())\n",
    "    print(f\"Targeting class '{TARGET_CLASS_NAME}' with ID: {TARGET_CLASS_ID} from {CUSTOM_MODEL_CLASS_NAMES}\")\n",
    "except ValueError:\n",
    "    print(f\"Error: Target class '{TARGET_CLASS_NAME}' not found in CUSTOM_MODEL_CLASS_NAMES: {CUSTOM_MODEL_CLASS_NAMES}\")\n",
    "    TARGET_CLASS_ID = None\n",
    "\n",
    "# --- Get Video Path  ---\n",
    "video_path = \"data/video/53.mp4\"\n",
    "if video_path == '0':\n",
    "    video_source = 0 # Use webcam\n",
    "else:\n",
    "    video_source = video_path"
   ],
   "id": "6f72e31c38ac1cf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targeting class 'carplate' with ID: 1 from ['__background__', 'carplate']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load FRCNN",
   "id": "625aa2ff895f70ae"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-11T12:40:15.274229Z",
     "start_time": "2025-05-11T12:40:14.798828Z"
    }
   },
   "source": [
    "nun_classes = 2\n",
    "frcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=nun_classes)\n",
    "frcnn.load_state_dict(torch.load('models/full_fasterrcnn_best.pth', map_location= 'cuda')['model_state_dict'])\n",
    "frcnn.to('cuda')\n",
    "frcnn.eval()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load EasyOCR",
   "id": "938b8a52f97918c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:40:15.305123Z",
     "start_time": "2025-05-11T12:40:15.290580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_config(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    opt = AttrDict(opt)\n",
    "    if opt.lang_char == 'None':\n",
    "        characters = ''\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            csv_path = os.path.join(opt['train_data'], data, 'label.csv')\n",
    "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character= ''.join(characters)\n",
    "    else:\n",
    "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
    "    return opt"
   ],
   "id": "bf4a2cef0a85e5cc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:40:15.336123Z",
     "start_time": "2025-05-11T12:40:15.321124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.EasyOCR.model import Model\n",
    "\n",
    "opt = get_config(\"models/EasyOCR/opt.txt\")\n",
    "\n",
    "if not opt.data_filtering_off:\n",
    "    print('Filtering the images containing characters which are not in opt.character')\n",
    "    print('Filtering the images whose label is longer than opt.batch_max_length')\n",
    "\n",
    "opt.select_data = opt.select_data.split('-')\n",
    "opt.batch_ratio = opt.batch_ratio.split('-')\n",
    "\n",
    "if 'CTC' in opt.Prediction:\n",
    "    converter = CTCLabelConverter(opt.character)\n",
    "else:\n",
    "    converter = AttnLabelConverter(opt.character)\n",
    "opt.num_class = len(converter.character)\n",
    "\n",
    "if opt.rgb:\n",
    "    opt.input_channel = 3"
   ],
   "id": "2ca0c2e586430ef",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:40:15.428123Z",
     "start_time": "2025-05-11T12:40:15.352125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "easyocr = Model(opt)\n",
    "print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "      opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "      opt.SequenceModeling, opt.Prediction)\n",
    "easyocr = torch.nn.DataParallel(easyocr).to(device)\n",
    "easyocr.load_state_dict(torch.load('./models/best_easyocr_full.pth', map_location = 'cuda'), strict=False)\n",
    "easyocr.eval()"
   ],
   "id": "e8f991f64a29a163",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Transformation module specified\n",
      "model input parameters 64 256 20 1 512 256 37 12 None VGG BiLSTM CTC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Model(\n",
       "    (FeatureExtraction): VGG_FeatureExtractor(\n",
       "      (ConvNet): Sequential(\n",
       "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "        (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (9): ReLU(inplace=True)\n",
       "        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "        (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (16): ReLU(inplace=True)\n",
       "        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "        (18): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (19): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
       "    (SequenceModeling): Sequential(\n",
       "      (0): BidirectionalLSTM(\n",
       "        (rnn): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
       "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): BidirectionalLSTM(\n",
       "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
       "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (Prediction): Linear(in_features=256, out_features=37, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Draw Predictions (Object Detection & OCR)",
   "id": "d25a8048795dc9fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T12:40:17.915547Z",
     "start_time": "2025-05-11T12:40:15.444632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Video Capture ---\n",
    "cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video source '{video_source}'. Please check the path or camera.\")\n",
    "else:\n",
    "    print(f\"Processing video: {video_source}\")\n",
    "    window_title = f\"Car Plate FRCNN Detection & Custom OCR\"\n",
    "    cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    ocr_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((opt.imgH, opt.imgW)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame_bgr = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            if isinstance(video_source, str): print(\"End of video file reached.\")\n",
    "            else: print(\"Error reading frame from webcam.\")\n",
    "            break\n",
    "\n",
    "        frame_to_draw = frame_bgr.copy()\n",
    "        frame_count += 1\n",
    "\n",
    "        img_rgb_frcnn = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        img_tensor_chw_frcnn = torch.from_numpy(img_rgb_frcnn.transpose((2, 0, 1)))\n",
    "        img_ready_for_frcnn = img_tensor_chw_frcnn.float().to(device) / 255.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            frcnn_outputs = frcnn([img_ready_for_frcnn])\n",
    "\n",
    "        predictions = frcnn_outputs[0]\n",
    "        pred_boxes = predictions['boxes'].cpu().numpy()\n",
    "        pred_labels = predictions['labels'].cpu().numpy()\n",
    "        pred_scores = predictions['scores'].cpu().numpy()\n",
    "\n",
    "        num_detections_in_frame = 0\n",
    "        if TARGET_CLASS_ID is not None:\n",
    "            for i in range(len(pred_scores)):\n",
    "                score = pred_scores[i]\n",
    "                label_id = pred_labels[i]\n",
    "                box = pred_boxes[i]\n",
    "\n",
    "                if label_id == TARGET_CLASS_ID and score >= CONFIDENCE_THRESHOLD:\n",
    "                    num_detections_in_frame += 1\n",
    "                    xmin, ymin, xmax, ymax = map(int, box)\n",
    "\n",
    "                    xmin = max(0, xmin); ymin = max(0, ymin)\n",
    "                    xmax = min(frame_bgr.shape[1], xmax); ymax = min(frame_bgr.shape[0], ymax)\n",
    "\n",
    "                    cv2.rectangle(frame_to_draw, (xmin, ymin), (xmax, ymax), BOX_COLOR, 2)\n",
    "                    class_name_to_display = CUSTOM_MODEL_CLASS_NAMES[label_id]\n",
    "                    label_text_frcnn = f\"{class_name_to_display}: {score:.2f}\"\n",
    "                    (text_w_frcnn, text_h_frcnn), base_frcnn = cv2.getTextSize(label_text_frcnn, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "                    cv2.rectangle(frame_to_draw, (xmin, ymin - text_h_frcnn - base_frcnn - 2), (xmin + text_w_frcnn, ymin - base_frcnn + 2), TEXT_BG_COLOR, -1)\n",
    "                    cv2.putText(frame_to_draw, label_text_frcnn, (xmin, ymin - base_frcnn -1), cv2.FONT_HERSHEY_SIMPLEX, 0.6, TEXT_COLOR, 1, cv2.LINE_AA)\n",
    "\n",
    "                    if xmax > xmin and ymax > ymin:\n",
    "                        plate_roi_bgr = frame_bgr[ymin:ymax, xmin:xmax]\n",
    "                        try:\n",
    "                            ocr_image_tensor = ocr_transform(plate_roi_bgr)\n",
    "                            ocr_image_tensor = ocr_image_tensor.unsqueeze(0).to(device) # [1, C, H, W]\n",
    "\n",
    "                            with torch.no_grad():\n",
    "                                ocr_preds_raw = easyocr(ocr_image_tensor, '')\n",
    "\n",
    "                            seq_len_T = ocr_preds_raw.size(1)\n",
    "                            if seq_len_T == 0:\n",
    "                                recognized_text = \"\"\n",
    "                                if frame_count % 10 == 0: print(f\"Frame {frame_count}: OCR skipped: 0-length sequence from model.\")\n",
    "                            else:\n",
    "                                _, ocr_preds_idx_batched = ocr_preds_raw.max(2)\n",
    "\n",
    "                                ocr_preds_idx_flattened = ocr_preds_idx_batched.view(-1)\n",
    "\n",
    "                                batch_size_ocr = ocr_image_tensor.size(0)\n",
    "                                preds_size_ocr = torch.IntTensor([seq_len_T] * batch_size_ocr).to(device)\n",
    "\n",
    "                                recognized_text_list = converter.decode_greedy(ocr_preds_idx_flattened.data, preds_size_ocr.data)\n",
    "                                recognized_text = recognized_text_list[0] if recognized_text_list else \"\"\n",
    "                                recognized_text = ''.join(filter(str.isalnum, recognized_text)).upper()\n",
    "                            if recognized_text:\n",
    "                                ocr_text_y_pos = ymax + 20\n",
    "                                if ocr_text_y_pos + 10 > frame_to_draw.shape[0]: ocr_text_y_pos = ymin - 10\n",
    "                                (text_w_ocr, text_h_ocr), base_ocr = cv2.getTextSize(recognized_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "                                cv2.rectangle(frame_to_draw, (xmin, ocr_text_y_pos - text_h_ocr - base_ocr), (xmin + text_w_ocr, ocr_text_y_pos + base_ocr), OCR_TEXT_BG_COLOR, -1)\n",
    "                                cv2.putText(frame_to_draw, recognized_text, (xmin, ocr_text_y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, OCR_TEXT_COLOR, 2)\n",
    "                                if frame_count % 10 == 0: print(f\"Frame {frame_count}: OCR Result: '{recognized_text}'\")\n",
    "                        except Exception as e:\n",
    "                            if frame_count % 10 == 0 or \"Boolean value of Tensor\" in str(e):\n",
    "                                print(f\"Frame {frame_count}: Error during OCR for a plate ROI: {e}\")\n",
    "                                if 'plate_roi_bgr' in locals(): print(f\"  Plate ROI shape: {plate_roi_bgr.shape}\")\n",
    "                                if 'ocr_preds_idx_batched' in locals() and 'preds_size_ocr' in locals() and seq_len_T > 0:\n",
    "                                     print(f\"  Input to decode_greedy (after view(-1).data): ocr_preds_idx_flattened shape: {ocr_preds_idx_flattened.shape}, preds_size_ocr: {preds_size_ocr.data}\")\n",
    "\n",
    "\n",
    "        if frame_count % 30 == 0: print(f\"Frame {frame_count}: Found {num_detections_in_frame} '{TARGET_CLASS_NAME}' instances by FRCNN.\")\n",
    "        cv2.imshow(window_title, frame_to_draw)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'): print(\"Exiting...\"); break\n",
    "\n",
    "    if cap.isOpened(): cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Video processing finished and resources released.\")"
   ],
   "id": "b5a9e7d47a89257e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: data/video/53.mp4\n",
      "Frame 10: OCR Result: 'QCH2377'\n",
      "Frame 20: OCR Result: 'QCH2347'\n",
      "Frame 30: OCR Result: 'QCH2347'\n",
      "Frame 30: Found 1 'carplate' instances by FRCNN.\n",
      "Exiting...\n",
      "Video processing finished and resources released.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
