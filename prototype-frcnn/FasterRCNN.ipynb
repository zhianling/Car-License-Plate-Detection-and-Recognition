{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "fc4b8fb2de37c597"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-18T03:18:32.436876Z",
     "start_time": "2025-03-18T03:18:30.712887Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from collections import defaultdict, deque\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from PIL import Image, ImageDraw  # Import ImageDraw"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Loading and Transformation",
   "id": "3175dffbfe4ddcec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:18:36.719627Z",
     "start_time": "2025-03-18T03:18:36.701630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, frame_dir, coco_dir, video_range, transform=None):\n",
    "        self.frame_dir = frame_dir\n",
    "        self.coco_dir = coco_dir\n",
    "        self.video_range = video_range\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        for video_num in range(self.video_range[0], self.video_range[1] + 1):\n",
    "            video_name = f\"video_{video_num:03d}\"\n",
    "            frame_subdir = os.path.join(self.frame_dir, video_name)\n",
    "            coco_subdir = os.path.join(self.coco_dir, f\"annotation_cvat_{video_num:03d}\")\n",
    "\n",
    "            if not os.path.exists(frame_subdir) or not os.path.exists(coco_subdir):\n",
    "                print(f\"Warning: Data for {video_name} not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            for coco_file in os.listdir(coco_subdir):\n",
    "                if not coco_file.endswith(\".json\"):\n",
    "                    continue\n",
    "\n",
    "                coco_path = os.path.join(coco_subdir, coco_file)\n",
    "                with open(coco_path, 'r') as f:\n",
    "                    coco_data = json.load(f)\n",
    "\n",
    "                frame_filename = coco_data['images'][0]['file_name']\n",
    "                frame_path = os.path.join(frame_subdir, frame_filename)\n",
    "\n",
    "                if not os.path.exists(frame_path):\n",
    "                    print(f\"Warning: Frame file {frame_filename} not found. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                annotations = coco_data['annotations']\n",
    "                boxes = []\n",
    "                labels = []\n",
    "                for ann in annotations:\n",
    "                    bbox = ann['bbox']\n",
    "                    boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
    "                    labels.append(ann['category_id'])\n",
    "\n",
    "                if not boxes:\n",
    "                    continue\n",
    "\n",
    "                data.append({\n",
    "                    'frame_path': frame_path,\n",
    "                    'boxes': boxes,\n",
    "                    'labels': labels\n",
    "                })\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        try:\n",
    "            img = Image.open(item['frame_path']).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR loading image: {item['frame_path']}, Error: {e}\")\n",
    "            raise\n",
    "\n",
    "        boxes = torch.as_tensor(item['boxes'], dtype=torch.float32)\n",
    "        labels = torch.as_tensor(item['labels'], dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "\n",
    "        if self.transform:\n",
    "            try:\n",
    "                img, target = self.transform(img, target)\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR applying transformations: {e}\")\n",
    "                raise\n",
    "\n",
    "        return img, target"
   ],
   "id": "3c11461bf62efb59",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:18:37.922678Z",
     "start_time": "2025-03-18T03:18:37.908669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        return F.to_tensor(image), target\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            image = F.hflip(image)\n",
    "            bbox = target[\"boxes\"]\n",
    "\n",
    "            bbox[:, [0, 2]] = image.size[0] - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "\n",
    "def get_transform(train=False):\n",
    "    transforms = []\n",
    "    transforms.append(ToTensor())\n",
    "    if train:\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    return Compose(transforms)"
   ],
   "id": "c317660f18c75da8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "525dd32509770c00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:18:48.116867Z",
     "start_time": "2025-03-18T03:18:48.048743Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    print(\"Training...\")\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    print(\"Setup Metrics Completed\")\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        print(\"Logging images to GPU\")\n",
    "        images = list(image.to(device) for image in images)\n",
    "        print(\"Logging targets to GPU\")\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        print(\"Load images and targets in GPU to the models\")\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = \"images:\"\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "class SmoothedValue:\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device=\"cuda\")\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value,\n",
    "        )\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\n",
    "            f\"'{type(self).__name__}' object has no attribute '{attr}'\"\n",
    "        )\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(f\"{name}: {str(meter)}\")\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = \"\"\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
    "        data_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
    "        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join(\n",
    "                [\n",
    "                    header,\n",
    "                    \"[{0\" + space_fmt + \"}/{1}]\",\n",
    "                    \"eta: {eta}\",\n",
    "                    \"{meters}\",\n",
    "                    \"time: {time}\",\n",
    "                    \"data: {data}\",\n",
    "                    \"max mem: {memory:.0f}\",\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            log_msg = self.delimiter.join(\n",
    "                [\n",
    "                    header,\n",
    "                    \"[{0\" + space_fmt + \"}/{1}]\",\n",
    "                    \"eta: {eta}\",\n",
    "                    \"{meters}\",\n",
    "                    \"time: {time}\",\n",
    "                    \"data: {data}\",\n",
    "                ]\n",
    "            )\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(\n",
    "                        log_msg.format(\n",
    "                            i,\n",
    "                            len(iterable),\n",
    "                            eta=eta_string,\n",
    "                            meters=str(self),\n",
    "                            time=str(iter_time),\n",
    "                            data=str(data_time),\n",
    "                            memory=torch.cuda.max_memory_allocated() / MB,\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        log_msg.format(\n",
    "                            i,\n",
    "                            len(iterable),\n",
    "                            eta=eta_string,\n",
    "                            meters=str(self),\n",
    "                            time=str(iter_time),\n",
    "                            data=str(data_time),\n",
    "                        )\n",
    "                    )\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print(\n",
    "            f\"{header} Total time: {total_time_str} ({total_time / len(iterable):.4f} s / it)\"\n",
    "        )\n",
    "\n",
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results.\n",
    "\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    with torch.inference_mode():\n",
    "        names = []\n",
    "        values = []\n",
    "        # sort the keys so that they are consistent across processes\n",
    "        for k in sorted(input_dict.keys()):\n",
    "            names.append(k)\n",
    "            values.append(input_dict[k])\n",
    "        values = torch.stack(values, dim=0)\n",
    "        dist.all_reduce(values)\n",
    "        if average:\n",
    "            values /= world_size\n",
    "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "    return reduced_dict\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()"
   ],
   "id": "a115ab25e74dc728"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Workflow",
   "id": "4d99e31a397a894e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:18:51.931401Z",
     "start_time": "2025-03-18T03:18:51.912950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # --- Configuration ---\n",
    "    frame_directory = \"./data/frame/\"\n",
    "    coco_annotation_directory = \"./data/annotation/coco/\"\n",
    "    num_classes = 2\n",
    "    batch_size = 4\n",
    "    num_workers = 0\n",
    "    num_epochs = 5\n",
    "    learning_rate = 0.005\n",
    "    momentum = 0.9\n",
    "    weight_decay = 0.0005\n",
    "    print_freq = 20\n",
    "\n",
    "    # --- Device ---\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Datasets ---\n",
    "    print(\"Loading dataset...\")\n",
    "    train_dataset = CocoDataset(frame_directory, coco_annotation_directory, (1, 7), get_transform())\n",
    "    test_dataset = CocoDataset(frame_directory, coco_annotation_directory, (8, 10), get_transform())\n",
    "\n",
    "    # --- DataLoaders ---\n",
    "    print(\"Adding dataset to dataloaders\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "    # --- TEST DATA LOADER ---\n",
    "    print(\"Testing DataLoader...\")\n",
    "    try:\n",
    "        for i, (images, targets) in enumerate(train_dataloader):\n",
    "            print(f\"Batch {i} loaded successfully.\")\n",
    "            print(f\"  Images shape: {images[0].shape}\")  # Check image shape\n",
    "            print(f\"  Targets: {targets}\") # Check the targets\n",
    "            break  # Only test the first batch\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading batch: {e}\")\n",
    "    print(\"DataLoader test complete.\")\n",
    "    # --- END TEST ---\n",
    "\n",
    "    # --- Model ---\n",
    "    print(\"Getting model from remote\")\n",
    "    model = get_model(num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # --- Optimizer ---\n",
    "    print(\"Setting up optimizer\")\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "        train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=print_freq)\n",
    "        # Evaluate after each epoch (optional, but good practice)\n",
    "        evaluate(model, test_dataloader, device)\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    # --- Save Model (optional) ---\n",
    "    torch.save(model.state_dict(), \"./output/faster_rcnn_model_epoch.pth\")\n",
    "    print(\"Model saved to output/faster_rcnn_model_epoch.pth\")"
   ],
   "id": "8b7643e8a6e8a3dd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "d746864d80f3b87e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Evaluation",
   "id": "d7d230c803eef34d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:30:42.843776Z",
     "start_time": "2025-03-18T03:30:42.815776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CocoDataset (Dataset):\n",
    "    def __init__ (self, frame_dir, coco_dir, video_range, transform = None):\n",
    "        self.frame_dir = frame_dir\n",
    "        self.coco_dir = coco_dir\n",
    "        self.video_range = video_range\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data ()\n",
    "\n",
    "    def _load_data (self):\n",
    "        data = []\n",
    "        for video_num in range (self.video_range[0], self.video_range[1] + 1):\n",
    "            video_name = f\"video_{video_num:03d}\"\n",
    "            frame_subdir = os.path.join (self.frame_dir, video_name)\n",
    "            coco_subdir = os.path.join (self.coco_dir, f\"annotation_cvat_{video_num:03d}\")\n",
    "\n",
    "            if not os.path.exists (frame_subdir) or not os.path.exists (coco_subdir):\n",
    "                print (f\"Warning: Data for {video_name} not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            for coco_file in os.listdir (coco_subdir):\n",
    "                if not coco_file.endswith (\".json\"):\n",
    "                    continue\n",
    "\n",
    "                coco_path = os.path.join (coco_subdir, coco_file)\n",
    "                with open (coco_path, 'r') as f:\n",
    "                    coco_data = json.load (f)\n",
    "\n",
    "                # Extract frame filename and remove extension\n",
    "                frame_filename = coco_data['images'][0]['file_name']\n",
    "                frame_path = os.path.join (frame_subdir, frame_filename)\n",
    "\n",
    "                if not os.path.exists (frame_path):\n",
    "                    print (f\"Warning: Frame file {frame_filename} not found. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                img = Image.open(frame_path) # Load here, to make it accessible to bounding box corrections.\n",
    "                img_width, img_height = img.size\n",
    "\n",
    "                annotations = coco_data['annotations']\n",
    "                boxes = []\n",
    "                labels = []\n",
    "                for ann in annotations:\n",
    "                    bbox = ann['bbox']\n",
    "                    boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
    "                    labels.append (ann['category_id'])\n",
    "\n",
    "                if not boxes:  # Skip images without annotations\n",
    "                    continue\n",
    "                image_id = coco_data['images'][0]['id']\n",
    "\n",
    "                data.append ({\n",
    "                    'frame_path': frame_path,\n",
    "                    'boxes': boxes,\n",
    "                    'labels': labels,\n",
    "                    'image_id': image_id,\n",
    "                })\n",
    "        return data\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len (self.data)\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        item = self.data[idx]\n",
    "        img = Image.open(item['frame_path']).convert(\"RGB\")\n",
    "        boxes = torch.as_tensor(item['boxes'], dtype=torch.float32)\n",
    "        labels = torch.as_tensor(item['labels'], dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])  # Add image_id here!\n",
    "\n",
    "        if self.transform:\n",
    "            img, target = self.transform(img, target)\n",
    "\n",
    "        return img, target"
   ],
   "id": "dbda727b49a9324d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:18:56.648379Z",
     "start_time": "2025-03-18T03:18:56.320928Z"
    }
   },
   "cell_type": "code",
   "source": "from torchmetrics.detection.mean_ap import MeanAveragePrecision",
   "id": "dfa1cff86735eeae",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:31:21.308032Z",
     "start_time": "2025-03-18T03:31:21.302032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    metric = MeanAveragePrecision(box_format=\"xyxy\", iou_type=\"bbox\", class_metrics=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            preds = []\n",
    "            for output in outputs:\n",
    "                preds.append(\n",
    "                    {\n",
    "                        \"boxes\": output[\"boxes\"].cpu(),\n",
    "                        \"scores\": output[\"scores\"].cpu(),\n",
    "                        \"labels\": output[\"labels\"].cpu(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            targets_formatted = []\n",
    "            for target in targets:\n",
    "                targets_formatted.append(\n",
    "                    {\n",
    "                        \"boxes\": target[\"boxes\"].cpu(),\n",
    "                        \"labels\": target[\"labels\"].cpu(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            metric.update(preds, targets_formatted)\n",
    "\n",
    "    result = metric.compute()\n",
    "    print(f\"TorchMetrics result inside evaluate: {result}\") #Add this line\n",
    "    return result"
   ],
   "id": "25bac5e5904accb",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    frame_directory = \"./data/frame/\"\n",
    "    coco_annotation_directory = \"./data/annotation/coco/\"\n",
    "    num_classes = 2\n",
    "    model_path = \"./output/faster_rcnn_model_5_epoch.pth\"\n",
    "    batch_size = 4\n",
    "    num_workers = 0\n",
    "\n",
    "    # --- Device ---\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    # --- Datasets ---\n",
    "    #  Only use the test dataset for evaluation\n",
    "    test_dataset = CocoDataset(frame_directory, coco_annotation_directory, (8, 11), get_transform())\n",
    "\n",
    "    # --- DataLoaders ---\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "    # --- Model ---\n",
    "    model = get_model(num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))  # Load the saved weights\n",
    "    model.to(device)\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    print(\"Evaluating performance...\")\n",
    "    print(evaluate(model, test_dataloader, device))"
   ],
   "id": "2a082084c94adbcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(image, predictions, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Visualizes predicted bounding boxes on an image.\n",
    "\n",
    "    Args:\n",
    "        image: PIL Image.\n",
    "        predictions: Dictionary with 'boxes', 'labels', and 'scores' keys.\n",
    "        threshold: Confidence threshold for displaying boxes.\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box, label, score in zip(predictions['boxes'], predictions['labels'], predictions['scores']):\n",
    "        if score >= threshold:\n",
    "            box = box.cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            label = label.cpu().numpy()\n",
    "            score = score.cpu().numpy()\n",
    "\n",
    "            x1, y1, x2, y2 = box\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "            draw.text((x1, y1 - 10), f\"Label: {label}, Score: {score:.2f}\", fill=\"red\")\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "index = 1750\n",
    "\n",
    "test_dataset = CocoDataset(frame_directory, coco_annotation_directory, (11, 11), get_transform())\n",
    "\n",
    "image, target = test_dataset[index]  # Get the first image and target\n",
    "image = image.to(device)\n",
    "# target = {k: v.to(device) for k,v in target.items()} # No need to send target to device for inference\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():  # Disable gradient calculations during inference\n",
    "    predictions = model([image])[0]  # Pass a list containing the image, get first element of result\n",
    "\n",
    "\n",
    "# Visualize the predictions\n",
    "original_image = Image.open(test_dataset.data[index]['frame_path']).convert(\"RGB\") # Load PIL Image\n",
    "visualize_predictions(original_image, predictions, threshold=0.5) #Visualize with threshold"
   ],
   "id": "699ad0719a8bc46a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
